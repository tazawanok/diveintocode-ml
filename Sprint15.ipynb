{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Sprint 論文読解入門完了課題\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (1) 物体検出の分野にはどういった手法が存在したか。\n",
    " There is a large literature on object\n",
    "proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in\n",
    "[19], [20], [21]. Widely used object proposal methods\n",
    "include those based on grouping super-pixels (e.g.,\n",
    "Selective Search [4], CPMC [22], MCG [23]) and those\n",
    "based on sliding windows (e.g., objectness in windows\n",
    "[24], EdgeBoxes [6]). Object proposal methods were\n",
    "adopted as external modules independent of the detectors (e.g., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [\n",
    "- R-CNN\n",
    "- Fast R-CNN\n",
    "- Selective Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    " \n",
    " State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
    "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
    "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
    "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
    "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
    "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
    "“attention” mechanisms, the RPN component tells the unified network where to look. \n",
    " \n",
    " - Fast R-CNN以前の手法では、物体領域抽出にSelective Searchというディープラーニング以前の手法を使っていた。抽出性能が低く、候補が無数に抽出されてしまい、物体候補領域の抽出とそのあとの識別処理の計算コストが膨大になってしまった。\n",
    " - 一方で、Faster R-CNNはSelective Searchの代わりに、Region Proposal Network(RPN)を導入した。end-to-endの学習・推定と、高性能な物体候補領域の抽出を実現した。そのため、高速かつ高性能な物体検出が可能となった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    " https://laboro.ai/docs/%E3%83%86%E3%82%99%E3%82%A3%E3%83%BC%E3%83%95%E3%82%9A%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%AF%E3%82%99%E3%81%AB%E3%82%88%E3%82%8B%E4%B8%80%E8%88%AC%E7%89%A9%E4%BD%93%E8%AA%8D%E8%AD%98%E3%81%A8%E3%81%9D%E3%81%AE%E3%83%92%E3%82%99%E3%82%B7%E3%82%99%E3%83%8D%E3%82%B9%E5%BF%9C%E7%94%A8.pdf\n",
    "  -  two-stage法は、物体が写っている領域（RPN）と、領域候補のカテゴリを識別するネットワークを直列に実行する方法である。代表的なモデルとしてFaster R-CNNやMask R-CNNがある。\n",
    "  - One-Stage法は、一つのネットワークで領域抽出をカテゴリ識別を同時に行う方法で、一般的に高速に動作する。代表的なモデルとして、SSD、Yolo、Retina Netがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (4) RPNとは何か。\n",
    " Our observation is that the convolutional feature\n",
    "maps used by region-based detectors, like Fast RCNN, can also be used for generating region proposals. On top of these convolutional features, we\n",
    "construct an RPN by adding a few additional convolutional layers that simultaneously regress region\n",
    "bounds and objectness scores at each location on a\n",
    "regular grid. The RPN is thus a kind of fully convolutional network (FCN) [7] and can be trained end-toend specifically for the task for generating detection\n",
    "proposals.\n",
    "RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. In\n",
    "contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters\n",
    "(Figure 1, b), we introduce novel “anchor” boxes\n",
    "that serve as references at multiple scales and aspect\n",
    "ratios. Our scheme can be thought of as a pyramid\n",
    "of regression references (Figure 1, c), which avoids\n",
    "enumerating images or filters of multiple scales or\n",
    "aspect ratios. This model performs well when trained\n",
    "and tested using single-scale images and thus benefits\n",
    "running speed.\n",
    "To unify RPNs with Fast R-CNN [2] object detection networks, we propose a training scheme that\n",
    "alternates between fine-tuning for the region proposal\n",
    "task and then fine-tuning for object detection, while\n",
    "keeping the proposals fixed. This scheme converges\n",
    "quickly and produces a unified network with convolutional features that are shared between both tasks.1\n",
    "We comprehensively evaluate our method on the\n",
    "PASCAL VOC detection benchmarks [11] where RPNs\n",
    "with Fast R-CNNs produce detection accuracy better than the strong baseline of Selective Search with\n",
    "Fast R-CNNs. Meanwhile, our method waives nearly\n",
    "all computational burdens of Selective Search at\n",
    "test-time—the effective running time for proposals\n",
    "is just 10 milliseconds. Using the expensive very\n",
    "deep models of [3], our detection method still has\n",
    "a frame rate of 5fps (including all steps) on a GPU,\n",
    "and thus is a practical object detection system in\n",
    "terms of both speed and accuracy. We also report\n",
    "results on the MS COCO dataset [12] and investigate the improvements on PASCAL VOC using the\n",
    "COCO data. Code has been made publicly available\n",
    "at https://github.com/shaoqingren/faster_\n",
    "rcnn (in MATLAB) and https://github.com/\n",
    "rbgirshick/py-faster-rcnn (in Python).\n",
    " <br>\n",
    " \n",
    " - 画像内に写る物体の境界線とスコア(IoU)を推定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (5) RoIプーリングとは何か。\n",
    " \n",
    " Given a box predicted by stage 1, we extract a feature of this box by Region-of-Interest (RoI) pooling [15, 9]. The purpose of RoI pooling is for producing a fixed-size feature from an arbitrary box, which is set as 14\n",
    "×\n",
    "14 at this stage. We append two extra fully-connected (fc) layers to this feature for each box. The first fc layer (with ReLU) reduces the dimension to 256, followed by the second fc layer that regresses a pixel-wise mask. This mask, of a pre-defined spatial resolution of \n",
    "m\n",
    "×\n",
    "m\n",
    " (we use \n",
    "m\n",
    "=\n",
    "28\n",
    "), is parameterized by an \n",
    "m\n",
    "2\n",
    "-dimensional vector. The second fc layer has \n",
    "m\n",
    "2\n",
    " outputs, each performing binary logistic regression to the ground truth mask.\n",
    " \n",
    " \n",
    "  - 畳み込み処理を行なったfeature mapからRegion proposalにあたる領域のpoolingを行う。そして、物体候補ごとに同じサイズのFeature mapを生成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (6) Anchorのサイズはどうするのが適切か。\n",
    " At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as \n",
    "k\n",
    ". So the reg layer has \n",
    "4\n",
    "k\n",
    " outputs encoding the coordinates of \n",
    "k\n",
    " boxes, and the cls layer outputs \n",
    "2\n",
    "k\n",
    " scores that estimate probability of object or not object for each proposal4. The \n",
    "k\n",
    " proposals are parameterized relative to \n",
    "k\n",
    " reference boxes, which we call anchors. An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding \n",
    "k\n",
    "=\n",
    "9\n",
    " anchors at each sliding position. For a convolutional feature map of a size \n",
    "W\n",
    "×\n",
    "H\n",
    " (typically \n",
    "∼\n",
    "2,400), there are \n",
    "W\n",
    "H\n",
    "k\n",
    " anchors in total.\n",
    " \n",
    " \n",
    " - デフォルトでは、3つのスケールと3つのアスペクト比を使用して、各スライド位置で\n",
    "k \n",
    "= \n",
    "9\n",
    "アンカーを生成します。\n",
    " - アンカーボックスは一つのアンカーに対して９つ作られる。アンカーボックスの面積は揃える必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate our Faster R-CNN system. Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast R-CNN counterpart under the same protocol (Table XI). This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster R-CNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on the COCO test-dev set. Figure 6 shows some results on the MS COCO test-dev set.\n",
    "\n",
    "\n",
    " - データセット：COCO\n",
    " - mAP@0.5: Fast R-CNNはFaster R-CNNと比較して2.8%高い\n",
    " - mAP@[.5, .95]:Fast R-CNNはFaster R-CNNと比較して2.2%高い\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
