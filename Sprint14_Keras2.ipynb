{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 【問題1】公式Exampleを分担して実行\n",
    "TensorFLowの公式Exampleを分担して実行してください。\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 次のチュートリアルを実装したが、学習が終わらなかった。\n",
    "<br>\n",
    "\n",
    "https://www.tensorflow.org/tutorials/text/image_captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:100]\n",
    "y = iris.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, X_test,\n",
    " y_train, y_test) = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0,\n",
    ")\n",
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 7\n",
      "Trainable params: 7\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/naoki/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5536\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 0s 181us/step - loss: 0.6715 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 0s 199us/step - loss: 0.6547 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 0s 205us/step - loss: 0.6369 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 0s 172us/step - loss: 0.6168 - accuracy: 0.5536\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 0s 196us/step - loss: 0.5976 - accuracy: 0.8393\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 0s 215us/step - loss: 0.5759 - accuracy: 0.9464\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 0s 203us/step - loss: 0.5548 - accuracy: 0.9464\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 0s 184us/step - loss: 0.5335 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 0s 165us/step - loss: 0.5138 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 0s 165us/step - loss: 0.4953 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 0s 160us/step - loss: 0.4756 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 0s 159us/step - loss: 0.4577 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 0s 167us/step - loss: 0.4406 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 0s 167us/step - loss: 0.4244 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 0s 163us/step - loss: 0.4089 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 0s 178us/step - loss: 0.3935 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 0s 161us/step - loss: 0.3794 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 0s 164us/step - loss: 0.3656 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 0s 177us/step - loss: 0.3529 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 0s 162us/step - loss: 0.3400 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 0s 173us/step - loss: 0.3285 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 0s 166us/step - loss: 0.3173 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 0s 165us/step - loss: 0.3068 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 0s 172us/step - loss: 0.2963 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 0s 156us/step - loss: 0.2866 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 0s 155us/step - loss: 0.2772 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 0s 163us/step - loss: 0.2682 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 0s 171us/step - loss: 0.2599 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 0s 162us/step - loss: 0.2516 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 0s 177us/step - loss: 0.2438 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 0s 167us/step - loss: 0.2361 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 0s 173us/step - loss: 0.2288 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 0s 167us/step - loss: 0.2222 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 0s 166us/step - loss: 0.2155 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 0s 167us/step - loss: 0.2092 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 0s 167us/step - loss: 0.2031 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 0s 163us/step - loss: 0.1972 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 0s 165us/step - loss: 0.1916 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 0s 172us/step - loss: 0.1862 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 0s 170us/step - loss: 0.1810 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 0s 168us/step - loss: 0.1761 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 0s 166us/step - loss: 0.1712 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 0s 188us/step - loss: 0.1666 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 0s 185us/step - loss: 0.1622 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 0s 191us/step - loss: 0.1580 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 0s 165us/step - loss: 0.1539 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 0s 160us/step - loss: 0.1499 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 0s 155us/step - loss: 0.1462 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 0s 174us/step - loss: 0.1426 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(4,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=5,\n",
    "                    epochs=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率:1.0\n",
      "再現率:1.0\n",
      "F値 :1.0\n",
      "[[ 9  0]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "y_pred_val = np.where(y_pred_proba_val >0.5, 1, 0)\n",
    "print(\"適合率:{}\".format(precision_score(y_val,y_pred_val)))\n",
    "print(\"再現率:{}\".format(recall_score(y_val,y_pred_val)))\n",
    "print(\"F値 :{}\".format(f1_score(y_val,y_pred_val)))\n",
    "print(confusion_matrix(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率:1.0\n",
      "再現率:1.0\n",
      "F値 :1.0\n",
      "[[13  0]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"適合率:{}\".format(precision_score(y_test,y_pred)))\n",
    "print(\"再現率:{}\".format(recall_score(y_test,y_pred)))\n",
    "print(\"F値 :{}\".format(f1_score(y_test,y_pred)))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/naoki/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 281\n",
      "Trainable params: 281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "56/56 [==============================] - 0s 2ms/sample - loss: 0.4409 - acc: 0.8214\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 0s 775us/sample - loss: 0.0488 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 0s 785us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 0s 814us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 0s 816us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 0s 833us/sample - loss: 8.4581e-04 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 0s 798us/sample - loss: 6.1813e-04 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 0s 815us/sample - loss: 4.8206e-04 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 0s 813us/sample - loss: 3.8052e-04 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 0s 812us/sample - loss: 3.0021e-04 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 0s 811us/sample - loss: 2.4978e-04 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 0s 790us/sample - loss: 2.1391e-04 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 0s 802us/sample - loss: 1.8639e-04 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 0s 846us/sample - loss: 1.6198e-04 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 0s 886us/sample - loss: 1.3380e-04 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 0s 791us/sample - loss: 1.1775e-04 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 0s 770us/sample - loss: 1.0777e-04 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 0s 776us/sample - loss: 9.3705e-05 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 0s 785us/sample - loss: 8.5956e-05 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 0s 800us/sample - loss: 7.8667e-05 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 0s 790us/sample - loss: 6.9906e-05 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 0s 811us/sample - loss: 6.3502e-05 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 0s 821us/sample - loss: 6.1054e-05 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 0s 800us/sample - loss: 5.1744e-05 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 0s 802us/sample - loss: 4.9305e-05 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 0s 814us/sample - loss: 4.5431e-05 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 0s 787us/sample - loss: 4.0717e-05 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 0s 852us/sample - loss: 3.6848e-05 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 0s 798us/sample - loss: 3.4637e-05 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 0s 800us/sample - loss: 3.2381e-05 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 0s 786us/sample - loss: 3.0331e-05 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 0s 691us/sample - loss: 2.7971e-05 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 0s 713us/sample - loss: 2.6357e-05 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 0s 687us/sample - loss: 2.5690e-05 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 0s 732us/sample - loss: 2.3224e-05 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 0s 809us/sample - loss: 2.2175e-05 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 0s 793us/sample - loss: 2.0736e-05 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 0s 782us/sample - loss: 1.8989e-05 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 0s 790us/sample - loss: 1.8336e-05 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 0s 747us/sample - loss: 1.6937e-05 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 0s 781us/sample - loss: 1.6083e-05 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 0s 755us/sample - loss: 1.5484e-05 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 0s 711us/sample - loss: 1.4450e-05 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 0s 742us/sample - loss: 1.3590e-05 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 0s 720us/sample - loss: 1.3035e-05 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 0s 729us/sample - loss: 1.2543e-05 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 0s 712us/sample - loss: 1.1682e-05 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 0s 741us/sample - loss: 1.1011e-05 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 0s 761us/sample - loss: 1.0654e-05 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 0s 741us/sample - loss: 1.0253e-05 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 0s 739us/sample - loss: 9.6458e-06 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 0s 738us/sample - loss: 9.3040e-06 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 0s 699us/sample - loss: 8.7701e-06 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 0s 694us/sample - loss: 8.4240e-06 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 0s 694us/sample - loss: 7.9583e-06 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 0s 765us/sample - loss: 7.6990e-06 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 0s 755us/sample - loss: 7.1992e-06 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 0s 760us/sample - loss: 6.9671e-06 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 0s 767us/sample - loss: 6.6703e-06 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 0s 765us/sample - loss: 6.3614e-06 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 0s 775us/sample - loss: 6.3253e-06 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 0s 771us/sample - loss: 5.8030e-06 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 0s 755us/sample - loss: 5.4858e-06 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 0s 787us/sample - loss: 5.4782e-06 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 0s 768us/sample - loss: 5.1503e-06 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 0s 755us/sample - loss: 4.8575e-06 - acc: 1.0000\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 766us/sample - loss: 4.7146e-06 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 0s 758us/sample - loss: 4.5173e-06 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 0s 796us/sample - loss: 4.3313e-06 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 0s 809us/sample - loss: 4.1157e-06 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 0s 788us/sample - loss: 4.1185e-06 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 0s 714us/sample - loss: 3.8331e-06 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 0s 756us/sample - loss: 3.7552e-06 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 0s 680us/sample - loss: 3.5149e-06 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 0s 711us/sample - loss: 3.4305e-06 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 0s 700us/sample - loss: 3.3317e-06 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 0s 749us/sample - loss: 3.2879e-06 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 0s 707us/sample - loss: 3.0249e-06 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 0s 722us/sample - loss: 2.9598e-06 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 0s 753us/sample - loss: 2.8165e-06 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 0s 714us/sample - loss: 2.7279e-06 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 0s 680us/sample - loss: 2.5875e-06 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 0s 718us/sample - loss: 2.5137e-06 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 0s 861us/sample - loss: 2.4581e-06 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 0s 828us/sample - loss: 2.3396e-06 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 0s 735us/sample - loss: 2.2635e-06 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 0s 738us/sample - loss: 2.2207e-06 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 0s 705us/sample - loss: 2.1251e-06 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 0s 735us/sample - loss: 2.0381e-06 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 0s 689us/sample - loss: 1.9474e-06 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 0s 711us/sample - loss: 1.8895e-06 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 0s 718us/sample - loss: 1.8005e-06 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 0s 744us/sample - loss: 1.7677e-06 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 0s 830us/sample - loss: 1.6909e-06 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 0s 824us/sample - loss: 1.6536e-06 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 0s 842us/sample - loss: 1.6581e-06 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 0s 789us/sample - loss: 1.5226e-06 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 0s 777us/sample - loss: 1.4634e-06 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 0s 772us/sample - loss: 1.4107e-06 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 0s 715us/sample - loss: 1.3604e-06 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=100,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率:1.0\n",
      "再現率:1.0\n",
      "F値 :1.0\n",
      "[[ 9  0]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "y_pred_val = np.where(y_pred_proba_val >0.5, 1, 0)\n",
    "print(\"適合率:{}\".format(precision_score(y_val,y_pred_val)))\n",
    "print(\"再現率:{}\".format(recall_score(y_val,y_pred_val)))\n",
    "print(\"F値 :{}\".format(f1_score(y_val,y_pred_val)))\n",
    "print(confusion_matrix(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率:1.0\n",
      "再現率:1.0\n",
      "F値 :1.0\n",
      "[[13  0]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"適合率:{}\".format(precision_score(y_test,y_pred)))\n",
    "print(\"再現率:{}\".format(recall_score(y_test,y_pred)))\n",
    "print(\"F値 :{}\".format(f1_score(y_test,y_pred)))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 【問題4】Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test,\n",
    " y_train, y_test) = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0,\n",
    ")\n",
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 303\n",
      "Trainable params: 303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.6969 - acc: 0.6667\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.2451 - acc: 0.9286\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 0s 683us/sample - loss: 0.3086 - acc: 0.8810\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 0s 667us/sample - loss: 0.2230 - acc: 0.8810\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.1384 - acc: 0.9405\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.1650 - acc: 0.9286\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.2584 - acc: 0.9286\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.2371 - acc: 0.9167\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 0s 695us/sample - loss: 0.2765 - acc: 0.9048\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 0s 665us/sample - loss: 0.3043 - acc: 0.8810\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 0s 683us/sample - loss: 0.0836 - acc: 0.9762\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.0840 - acc: 0.9643\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 0s 676us/sample - loss: 0.2147 - acc: 0.9286\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.2239 - acc: 0.9167\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.1840 - acc: 0.9405\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.1237 - acc: 0.9643\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.1311 - acc: 0.9405\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.2386 - acc: 0.9405\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 0s 689us/sample - loss: 0.2840 - acc: 0.8929\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0975 - acc: 0.9524\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.0680 - acc: 0.9762\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 0s 681us/sample - loss: 0.1171 - acc: 0.9524\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 0s 660us/sample - loss: 0.0367 - acc: 0.9881\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 0s 662us/sample - loss: 0.2469 - acc: 0.9048\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.0800 - acc: 0.9762\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 0s 661us/sample - loss: 0.1145 - acc: 0.9286\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 0s 687us/sample - loss: 0.1084 - acc: 0.9524\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 0s 665us/sample - loss: 0.1154 - acc: 0.9643\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.1092 - acc: 0.9524\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 0s 674us/sample - loss: 0.2464 - acc: 0.8571\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 0s 678us/sample - loss: 0.1080 - acc: 0.9524\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 0s 690us/sample - loss: 0.0747 - acc: 0.9643\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.1182 - acc: 0.9405\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 0s 667us/sample - loss: 0.0951 - acc: 0.9405\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 0s 689us/sample - loss: 0.0854 - acc: 0.9643\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 0s 657us/sample - loss: 0.1059 - acc: 0.9524\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.0966 - acc: 0.9643\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 0s 675us/sample - loss: 0.0876 - acc: 0.9643\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.1059 - acc: 0.9762\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 0s 666us/sample - loss: 0.1089 - acc: 0.9524\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.0731 - acc: 0.9881\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 0s 668us/sample - loss: 0.3083 - acc: 0.9286\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0694 - acc: 0.9762\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.1420 - acc: 0.9762\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 0s 732us/sample - loss: 0.1510 - acc: 0.9524\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.1322 - acc: 0.9405\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0849 - acc: 0.9524\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.1137 - acc: 0.9643\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 0s 766us/sample - loss: 0.1132 - acc: 0.9405\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 0s 768us/sample - loss: 0.0490 - acc: 0.9762\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.1232 - acc: 0.9762\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.1150 - acc: 0.9405\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 0s 780us/sample - loss: 0.0772 - acc: 0.9762\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 0s 776us/sample - loss: 0.1475 - acc: 0.9405\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0929 - acc: 0.9643\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.0886 - acc: 0.9643\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0872 - acc: 0.9881\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.1167 - acc: 0.9286\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 0s 814us/sample - loss: 0.1483 - acc: 0.9405\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.1105 - acc: 0.9405\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0813 - acc: 0.9643\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.0828 - acc: 0.9643\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 0s 801us/sample - loss: 0.1353 - acc: 0.9405\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 0s 791us/sample - loss: 0.1149 - acc: 0.9286\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 0s 742us/sample - loss: 0.1168 - acc: 0.9524\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.1111 - acc: 0.9643\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0683 - acc: 0.9762\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.1246 - acc: 0.9524\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.1052 - acc: 0.9643\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0928 - acc: 0.9643\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.1634 - acc: 0.9405\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0588 - acc: 0.9762\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 907us/sample - loss: 0.0909 - acc: 0.9405\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 0s 885us/sample - loss: 0.1251 - acc: 0.9405\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0887 - acc: 0.9762\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 0s 674us/sample - loss: 0.1538 - acc: 0.9167\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 0s 667us/sample - loss: 0.0635 - acc: 0.9643\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 0s 687us/sample - loss: 0.0803 - acc: 0.9762\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 0s 661us/sample - loss: 0.1678 - acc: 0.9524\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0664 - acc: 0.9643\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0859 - acc: 0.9762\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.1770 - acc: 0.9286\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0851 - acc: 0.9643\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 0s 669us/sample - loss: 0.0784 - acc: 0.9881\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.1237 - acc: 0.9405\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0750 - acc: 0.9762\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.1124 - acc: 0.9405\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0852 - acc: 0.9524\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.1718 - acc: 0.9167\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 0s 660us/sample - loss: 0.0688 - acc: 0.9762\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.0685 - acc: 0.9762\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 0s 658us/sample - loss: 0.1550 - acc: 0.9286\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 0s 686us/sample - loss: 0.0733 - acc: 0.9643\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.1096 - acc: 0.9405\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0848 - acc: 0.9524\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0408 - acc: 0.9881\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 0s 668us/sample - loss: 0.0605 - acc: 0.9762\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 0s 689us/sample - loss: 0.1103 - acc: 0.9643\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 0s 667us/sample - loss: 0.0702 - acc: 0.9762\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 0s 682us/sample - loss: 0.0895 - acc: 0.9643\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 0s 669us/sample - loss: 0.3638 - acc: 0.8929\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0798 - acc: 0.9762\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 0s 677us/sample - loss: 0.0668 - acc: 0.9881\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 0s 683us/sample - loss: 0.1147 - acc: 0.9643\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 0s 658us/sample - loss: 0.0611 - acc: 0.9762\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.2136 - acc: 0.9286\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 0s 673us/sample - loss: 0.0871 - acc: 0.9643\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 0s 691us/sample - loss: 0.0812 - acc: 0.9643\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0663 - acc: 0.9643\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.0704 - acc: 0.9762\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0784 - acc: 0.9643\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 0s 691us/sample - loss: 0.1141 - acc: 0.9405\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 0s 654us/sample - loss: 0.0746 - acc: 0.9643\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0493 - acc: 0.9881\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.0678 - acc: 0.9762\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0951 - acc: 0.9881\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.1606 - acc: 0.9405\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 0s 661us/sample - loss: 0.0615 - acc: 0.9643\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.1125 - acc: 0.9762\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 0s 681us/sample - loss: 0.0800 - acc: 0.9643\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0570 - acc: 0.9762\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.0767 - acc: 0.9524\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0671 - acc: 0.9762\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 0s 684us/sample - loss: 0.0649 - acc: 0.9643\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 0s 660us/sample - loss: 0.0801 - acc: 0.9762\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0553 - acc: 0.9762\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 0s 667us/sample - loss: 0.0769 - acc: 0.9643\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 0s 680us/sample - loss: 0.0683 - acc: 0.9643\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 0s 690us/sample - loss: 0.0799 - acc: 0.9643\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 0s 668us/sample - loss: 0.0850 - acc: 0.9643\n",
      "Epoch 131/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0858 - acc: 0.9762\n",
      "Epoch 132/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.1004 - acc: 0.9405\n",
      "Epoch 133/1000\n",
      "84/84 [==============================] - 0s 682us/sample - loss: 0.1343 - acc: 0.9405\n",
      "Epoch 134/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0931 - acc: 0.9643\n",
      "Epoch 135/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.1185 - acc: 0.9167\n",
      "Epoch 136/1000\n",
      "84/84 [==============================] - 0s 830us/sample - loss: 0.0693 - acc: 0.9643\n",
      "Epoch 137/1000\n",
      "84/84 [==============================] - 0s 820us/sample - loss: 0.0941 - acc: 0.9643\n",
      "Epoch 138/1000\n",
      "84/84 [==============================] - 0s 781us/sample - loss: 0.0665 - acc: 0.9643\n",
      "Epoch 139/1000\n",
      "84/84 [==============================] - 0s 859us/sample - loss: 0.0666 - acc: 0.9524\n",
      "Epoch 140/1000\n",
      "84/84 [==============================] - 0s 924us/sample - loss: 0.1115 - acc: 0.9524\n",
      "Epoch 141/1000\n",
      "84/84 [==============================] - 0s 849us/sample - loss: 0.0681 - acc: 0.9643\n",
      "Epoch 142/1000\n",
      "84/84 [==============================] - 0s 872us/sample - loss: 0.1542 - acc: 0.9167\n",
      "Epoch 143/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.0750 - acc: 0.9643\n",
      "Epoch 144/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0902 - acc: 0.9762\n",
      "Epoch 145/1000\n",
      "84/84 [==============================] - 0s 676us/sample - loss: 0.0618 - acc: 0.9762\n",
      "Epoch 146/1000\n",
      "84/84 [==============================] - 0s 673us/sample - loss: 0.1068 - acc: 0.9405\n",
      "Epoch 147/1000\n",
      "84/84 [==============================] - 0s 692us/sample - loss: 0.0780 - acc: 0.9762\n",
      "Epoch 148/1000\n",
      "84/84 [==============================] - 0s 679us/sample - loss: 0.0830 - acc: 0.9524\n",
      "Epoch 149/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.0531 - acc: 0.9762\n",
      "Epoch 150/1000\n",
      "84/84 [==============================] - 0s 674us/sample - loss: 0.0720 - acc: 0.9881\n",
      "Epoch 151/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0690 - acc: 0.9643\n",
      "Epoch 152/1000\n",
      "84/84 [==============================] - 0s 687us/sample - loss: 0.1065 - acc: 0.9405\n",
      "Epoch 153/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0384 - acc: 0.9762\n",
      "Epoch 154/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 679us/sample - loss: 0.1379 - acc: 0.9405\n",
      "Epoch 155/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0617 - acc: 0.9762\n",
      "Epoch 156/1000\n",
      "84/84 [==============================] - 0s 687us/sample - loss: 0.0697 - acc: 0.9643\n",
      "Epoch 157/1000\n",
      "84/84 [==============================] - 0s 683us/sample - loss: 0.0692 - acc: 0.9762\n",
      "Epoch 158/1000\n",
      "84/84 [==============================] - 0s 687us/sample - loss: 0.0543 - acc: 0.9762\n",
      "Epoch 159/1000\n",
      "84/84 [==============================] - 0s 660us/sample - loss: 0.0849 - acc: 0.9643\n",
      "Epoch 160/1000\n",
      "84/84 [==============================] - 0s 683us/sample - loss: 0.0616 - acc: 0.9643\n",
      "Epoch 161/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.0677 - acc: 0.9762\n",
      "Epoch 162/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.2480 - acc: 0.9167\n",
      "Epoch 163/1000\n",
      "84/84 [==============================] - 0s 664us/sample - loss: 0.0958 - acc: 0.9643\n",
      "Epoch 164/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0630 - acc: 0.9762\n",
      "Epoch 165/1000\n",
      "84/84 [==============================] - 0s 678us/sample - loss: 0.0772 - acc: 0.9524\n",
      "Epoch 166/1000\n",
      "84/84 [==============================] - 0s 682us/sample - loss: 0.0614 - acc: 0.9643\n",
      "Epoch 167/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0718 - acc: 0.9762\n",
      "Epoch 168/1000\n",
      "84/84 [==============================] - 0s 672us/sample - loss: 0.0618 - acc: 0.9762\n",
      "Epoch 169/1000\n",
      "84/84 [==============================] - 0s 897us/sample - loss: 0.0815 - acc: 0.9524\n",
      "Epoch 170/1000\n",
      "84/84 [==============================] - 0s 941us/sample - loss: 0.0594 - acc: 0.9762\n",
      "Epoch 171/1000\n",
      "84/84 [==============================] - 0s 851us/sample - loss: 0.0878 - acc: 0.9643\n",
      "Epoch 172/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.0515 - acc: 0.9762\n",
      "Epoch 173/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0390 - acc: 0.9762\n",
      "Epoch 174/1000\n",
      "84/84 [==============================] - 0s 751us/sample - loss: 0.0592 - acc: 0.9762\n",
      "Epoch 175/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0300 - acc: 0.9881\n",
      "Epoch 176/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0240 - acc: 0.9881\n",
      "Epoch 177/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.1260 - acc: 0.9643\n",
      "Epoch 178/1000\n",
      "84/84 [==============================] - 0s 939us/sample - loss: 0.0745 - acc: 0.9643\n",
      "Epoch 179/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0798 - acc: 0.9762\n",
      "Epoch 180/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0763 - acc: 0.9762\n",
      "Epoch 181/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.1406 - acc: 0.9643\n",
      "Epoch 182/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.1712 - acc: 0.9405\n",
      "Epoch 183/1000\n",
      "84/84 [==============================] - 0s 802us/sample - loss: 0.0886 - acc: 0.9405\n",
      "Epoch 184/1000\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0633 - acc: 0.9762\n",
      "Epoch 185/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.1187 - acc: 0.9524\n",
      "Epoch 186/1000\n",
      "84/84 [==============================] - 0s 830us/sample - loss: 0.0687 - acc: 0.9762\n",
      "Epoch 187/1000\n",
      "84/84 [==============================] - 0s 865us/sample - loss: 0.0556 - acc: 0.9881\n",
      "Epoch 188/1000\n",
      "84/84 [==============================] - 0s 836us/sample - loss: 0.0621 - acc: 0.9524\n",
      "Epoch 189/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.1837 - acc: 0.9286\n",
      "Epoch 190/1000\n",
      "84/84 [==============================] - 0s 773us/sample - loss: 0.2710 - acc: 0.9286\n",
      "Epoch 191/1000\n",
      "84/84 [==============================] - 0s 790us/sample - loss: 0.1160 - acc: 0.9405\n",
      "Epoch 192/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0540 - acc: 0.9762\n",
      "Epoch 193/1000\n",
      "84/84 [==============================] - 0s 698us/sample - loss: 0.0822 - acc: 0.9762\n",
      "Epoch 194/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0689 - acc: 0.9524\n",
      "Epoch 195/1000\n",
      "84/84 [==============================] - 0s 667us/sample - loss: 0.0671 - acc: 0.9643\n",
      "Epoch 196/1000\n",
      "84/84 [==============================] - 0s 769us/sample - loss: 0.0325 - acc: 0.9762\n",
      "Epoch 197/1000\n",
      "84/84 [==============================] - 0s 772us/sample - loss: 0.1384 - acc: 0.9405\n",
      "Epoch 198/1000\n",
      "84/84 [==============================] - 0s 753us/sample - loss: 0.0759 - acc: 0.9524\n",
      "Epoch 199/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0973 - acc: 0.9524\n",
      "Epoch 200/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.0883 - acc: 0.9524\n",
      "Epoch 201/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0672 - acc: 0.9405\n",
      "Epoch 202/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0718 - acc: 0.9524\n",
      "Epoch 203/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.0604 - acc: 0.9762\n",
      "Epoch 204/1000\n",
      "84/84 [==============================] - 0s 690us/sample - loss: 0.0789 - acc: 0.9643\n",
      "Epoch 205/1000\n",
      "84/84 [==============================] - 0s 665us/sample - loss: 0.0924 - acc: 0.9643\n",
      "Epoch 206/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0745 - acc: 0.9762\n",
      "Epoch 207/1000\n",
      "84/84 [==============================] - 0s 670us/sample - loss: 0.0883 - acc: 0.9643\n",
      "Epoch 208/1000\n",
      "84/84 [==============================] - 0s 701us/sample - loss: 0.0618 - acc: 0.9762\n",
      "Epoch 209/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.0576 - acc: 0.9524\n",
      "Epoch 210/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.1003 - acc: 0.9643\n",
      "Epoch 211/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0781 - acc: 0.9524\n",
      "Epoch 212/1000\n",
      "84/84 [==============================] - 0s 673us/sample - loss: 0.0553 - acc: 0.9762\n",
      "Epoch 213/1000\n",
      "84/84 [==============================] - 0s 698us/sample - loss: 0.0579 - acc: 0.9643\n",
      "Epoch 214/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.1065 - acc: 0.9524\n",
      "Epoch 215/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0830 - acc: 0.9643\n",
      "Epoch 216/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0591 - acc: 0.9643\n",
      "Epoch 217/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.1618 - acc: 0.9405\n",
      "Epoch 218/1000\n",
      "84/84 [==============================] - 0s 831us/sample - loss: 0.0585 - acc: 0.9881\n",
      "Epoch 219/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0466 - acc: 1.0000\n",
      "Epoch 220/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0465 - acc: 0.9762\n",
      "Epoch 221/1000\n",
      "84/84 [==============================] - 0s 993us/sample - loss: 0.1788 - acc: 0.9524\n",
      "Epoch 222/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0676 - acc: 0.9405\n",
      "Epoch 223/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0641 - acc: 0.9643\n",
      "Epoch 224/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0807 - acc: 0.9524\n",
      "Epoch 225/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0994 - acc: 0.9524\n",
      "Epoch 226/1000\n",
      "84/84 [==============================] - 0s 870us/sample - loss: 0.0719 - acc: 0.9762\n",
      "Epoch 227/1000\n",
      "84/84 [==============================] - 0s 798us/sample - loss: 0.0553 - acc: 0.9881\n",
      "Epoch 228/1000\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0582 - acc: 0.9643\n",
      "Epoch 229/1000\n",
      "84/84 [==============================] - 0s 817us/sample - loss: 0.1389 - acc: 0.9405\n",
      "Epoch 230/1000\n",
      "84/84 [==============================] - 0s 774us/sample - loss: 0.0672 - acc: 0.9405\n",
      "Epoch 231/1000\n",
      "84/84 [==============================] - 0s 806us/sample - loss: 0.0745 - acc: 0.9643\n",
      "Epoch 232/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0740 - acc: 0.9524\n",
      "Epoch 233/1000\n",
      "84/84 [==============================] - 0s 900us/sample - loss: 0.0601 - acc: 0.9762\n",
      "Epoch 234/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0764 - acc: 0.9643\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 979us/sample - loss: 0.0497 - acc: 0.9643\n",
      "Epoch 236/1000\n",
      "84/84 [==============================] - 0s 870us/sample - loss: 0.0652 - acc: 0.9762\n",
      "Epoch 237/1000\n",
      "84/84 [==============================] - 0s 804us/sample - loss: 0.0597 - acc: 0.9762\n",
      "Epoch 238/1000\n",
      "84/84 [==============================] - 0s 773us/sample - loss: 0.0727 - acc: 0.9643\n",
      "Epoch 239/1000\n",
      "84/84 [==============================] - 0s 804us/sample - loss: 0.0555 - acc: 0.9762\n",
      "Epoch 240/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0543 - acc: 0.9643\n",
      "Epoch 241/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.1033 - acc: 0.9405\n",
      "Epoch 242/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0684 - acc: 0.9643\n",
      "Epoch 243/1000\n",
      "84/84 [==============================] - 0s 757us/sample - loss: 0.1309 - acc: 0.9762\n",
      "Epoch 244/1000\n",
      "84/84 [==============================] - 0s 751us/sample - loss: 0.0794 - acc: 0.9643\n",
      "Epoch 245/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.1091 - acc: 0.9643\n",
      "Epoch 246/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0665 - acc: 0.9881\n",
      "Epoch 247/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0809 - acc: 0.9762\n",
      "Epoch 248/1000\n",
      "84/84 [==============================] - 0s 764us/sample - loss: 0.0851 - acc: 0.9643\n",
      "Epoch 249/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0672 - acc: 0.9643\n",
      "Epoch 250/1000\n",
      "84/84 [==============================] - 0s 769us/sample - loss: 0.0475 - acc: 0.9643\n",
      "Epoch 251/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.2332 - acc: 0.9524\n",
      "Epoch 252/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.1049 - acc: 0.9643\n",
      "Epoch 253/1000\n",
      "84/84 [==============================] - 0s 773us/sample - loss: 0.0615 - acc: 0.9762\n",
      "Epoch 254/1000\n",
      "84/84 [==============================] - 0s 774us/sample - loss: 0.0590 - acc: 0.9762\n",
      "Epoch 255/1000\n",
      "84/84 [==============================] - 0s 835us/sample - loss: 0.0821 - acc: 0.9405\n",
      "Epoch 256/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.1983 - acc: 0.9762\n",
      "Epoch 257/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.0882 - acc: 0.9762\n",
      "Epoch 258/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0797 - acc: 0.9643\n",
      "Epoch 259/1000\n",
      "84/84 [==============================] - 0s 736us/sample - loss: 0.0806 - acc: 0.9643\n",
      "Epoch 260/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0695 - acc: 0.9643\n",
      "Epoch 261/1000\n",
      "84/84 [==============================] - 0s 742us/sample - loss: 0.0744 - acc: 0.9643\n",
      "Epoch 262/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0645 - acc: 0.9643\n",
      "Epoch 263/1000\n",
      "84/84 [==============================] - 0s 762us/sample - loss: 0.0838 - acc: 0.9524\n",
      "Epoch 264/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.0822 - acc: 0.9643\n",
      "Epoch 265/1000\n",
      "84/84 [==============================] - 0s 781us/sample - loss: 0.0401 - acc: 0.9881\n",
      "Epoch 266/1000\n",
      "84/84 [==============================] - 0s 766us/sample - loss: 0.0753 - acc: 0.9762\n",
      "Epoch 267/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0982 - acc: 0.9524\n",
      "Epoch 268/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0602 - acc: 0.9762\n",
      "Epoch 269/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0748 - acc: 0.9643\n",
      "Epoch 270/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.0472 - acc: 0.9762\n",
      "Epoch 271/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0456 - acc: 0.9881\n",
      "Epoch 272/1000\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.0901 - acc: 0.9643\n",
      "Epoch 273/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0569 - acc: 0.9524\n",
      "Epoch 274/1000\n",
      "84/84 [==============================] - 0s 762us/sample - loss: 0.2557 - acc: 0.9405\n",
      "Epoch 275/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.1608 - acc: 0.9405\n",
      "Epoch 276/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0716 - acc: 0.9762\n",
      "Epoch 277/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0581 - acc: 0.9762\n",
      "Epoch 278/1000\n",
      "84/84 [==============================] - 0s 762us/sample - loss: 0.0591 - acc: 0.9643\n",
      "Epoch 279/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0690 - acc: 0.9524\n",
      "Epoch 280/1000\n",
      "84/84 [==============================] - 0s 767us/sample - loss: 0.2277 - acc: 0.9286\n",
      "Epoch 281/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.0877 - acc: 0.9405\n",
      "Epoch 282/1000\n",
      "84/84 [==============================] - 0s 783us/sample - loss: 0.0614 - acc: 0.9762\n",
      "Epoch 283/1000\n",
      "84/84 [==============================] - 0s 882us/sample - loss: 0.0542 - acc: 0.9643\n",
      "Epoch 284/1000\n",
      "84/84 [==============================] - 0s 837us/sample - loss: 0.0562 - acc: 1.0000\n",
      "Epoch 285/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0692 - acc: 0.9524\n",
      "Epoch 286/1000\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.0691 - acc: 0.9762\n",
      "Epoch 287/1000\n",
      "84/84 [==============================] - 0s 786us/sample - loss: 0.0485 - acc: 0.9643\n",
      "Epoch 288/1000\n",
      "84/84 [==============================] - 0s 858us/sample - loss: 0.0864 - acc: 0.9643\n",
      "Epoch 289/1000\n",
      "84/84 [==============================] - 0s 890us/sample - loss: 0.0826 - acc: 0.9643\n",
      "Epoch 290/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.0797 - acc: 0.9524\n",
      "Epoch 291/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0783 - acc: 0.9643\n",
      "Epoch 292/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0532 - acc: 0.9643\n",
      "Epoch 293/1000\n",
      "84/84 [==============================] - 0s 810us/sample - loss: 0.0638 - acc: 0.9762\n",
      "Epoch 294/1000\n",
      "84/84 [==============================] - 0s 940us/sample - loss: 0.0848 - acc: 0.9643\n",
      "Epoch 295/1000\n",
      "84/84 [==============================] - 0s 792us/sample - loss: 0.0744 - acc: 0.9643\n",
      "Epoch 296/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0851 - acc: 0.9762\n",
      "Epoch 297/1000\n",
      "84/84 [==============================] - 0s 768us/sample - loss: 0.0689 - acc: 0.9643\n",
      "Epoch 298/1000\n",
      "84/84 [==============================] - 0s 797us/sample - loss: 0.0881 - acc: 0.9524\n",
      "Epoch 299/1000\n",
      "84/84 [==============================] - 0s 906us/sample - loss: 0.0810 - acc: 0.9524\n",
      "Epoch 300/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0703 - acc: 0.9643\n",
      "Epoch 301/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0531 - acc: 0.9643\n",
      "Epoch 302/1000\n",
      "84/84 [==============================] - 0s 815us/sample - loss: 0.0581 - acc: 0.9643\n",
      "Epoch 303/1000\n",
      "84/84 [==============================] - 0s 882us/sample - loss: 0.0781 - acc: 0.9643\n",
      "Epoch 304/1000\n",
      "84/84 [==============================] - 0s 775us/sample - loss: 0.0748 - acc: 0.9524\n",
      "Epoch 305/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.0793 - acc: 0.9524\n",
      "Epoch 306/1000\n",
      "84/84 [==============================] - 0s 753us/sample - loss: 0.0513 - acc: 0.9762\n",
      "Epoch 307/1000\n",
      "84/84 [==============================] - 0s 834us/sample - loss: 0.0756 - acc: 0.9524\n",
      "Epoch 308/1000\n",
      "84/84 [==============================] - 0s 846us/sample - loss: 0.0989 - acc: 0.9524\n",
      "Epoch 309/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0658 - acc: 0.9524\n",
      "Epoch 310/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0685 - acc: 0.9762\n",
      "Epoch 311/1000\n",
      "84/84 [==============================] - 0s 786us/sample - loss: 0.0644 - acc: 0.9881\n",
      "Epoch 312/1000\n",
      "84/84 [==============================] - 0s 799us/sample - loss: 0.0332 - acc: 0.9881\n",
      "Epoch 313/1000\n",
      "84/84 [==============================] - 0s 782us/sample - loss: 0.0729 - acc: 0.9762\n",
      "Epoch 314/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.0955 - acc: 0.9762\n",
      "Epoch 315/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.1067 - acc: 0.9405\n",
      "Epoch 316/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0519 - acc: 0.9762\n",
      "Epoch 317/1000\n",
      "84/84 [==============================] - 0s 742us/sample - loss: 0.1584 - acc: 0.9405\n",
      "Epoch 318/1000\n",
      "84/84 [==============================] - 0s 762us/sample - loss: 0.1290 - acc: 0.9524\n",
      "Epoch 319/1000\n",
      "84/84 [==============================] - 0s 773us/sample - loss: 0.0622 - acc: 0.9762\n",
      "Epoch 320/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0445 - acc: 0.9881\n",
      "Epoch 321/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.1041 - acc: 0.9643\n",
      "Epoch 322/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.1171 - acc: 0.9405\n",
      "Epoch 323/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0522 - acc: 0.9643\n",
      "Epoch 324/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0828 - acc: 0.9524\n",
      "Epoch 325/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0717 - acc: 0.9643\n",
      "Epoch 326/1000\n",
      "84/84 [==============================] - 0s 810us/sample - loss: 0.0758 - acc: 0.9643\n",
      "Epoch 327/1000\n",
      "84/84 [==============================] - 0s 833us/sample - loss: 0.0554 - acc: 0.9762\n",
      "Epoch 328/1000\n",
      "84/84 [==============================] - 0s 822us/sample - loss: 0.0691 - acc: 0.9524\n",
      "Epoch 329/1000\n",
      "84/84 [==============================] - 0s 887us/sample - loss: 0.0588 - acc: 0.9762\n",
      "Epoch 330/1000\n",
      "84/84 [==============================] - 0s 926us/sample - loss: 0.0711 - acc: 0.9762\n",
      "Epoch 331/1000\n",
      "84/84 [==============================] - 0s 808us/sample - loss: 0.0780 - acc: 0.9643\n",
      "Epoch 332/1000\n",
      "84/84 [==============================] - 0s 843us/sample - loss: 0.0479 - acc: 0.9762\n",
      "Epoch 333/1000\n",
      "84/84 [==============================] - 0s 919us/sample - loss: 0.0542 - acc: 0.9762\n",
      "Epoch 334/1000\n",
      "84/84 [==============================] - 0s 990us/sample - loss: 0.1112 - acc: 0.9524\n",
      "Epoch 335/1000\n",
      "84/84 [==============================] - 0s 880us/sample - loss: 0.1451 - acc: 0.9524\n",
      "Epoch 336/1000\n",
      "84/84 [==============================] - 0s 939us/sample - loss: 0.0568 - acc: 0.9762\n",
      "Epoch 337/1000\n",
      "84/84 [==============================] - 0s 816us/sample - loss: 0.0695 - acc: 0.9762\n",
      "Epoch 338/1000\n",
      "84/84 [==============================] - 0s 829us/sample - loss: 0.0580 - acc: 0.9762\n",
      "Epoch 339/1000\n",
      "84/84 [==============================] - 0s 853us/sample - loss: 0.0571 - acc: 0.9762\n",
      "Epoch 340/1000\n",
      "84/84 [==============================] - 0s 824us/sample - loss: 0.0604 - acc: 0.9762\n",
      "Epoch 341/1000\n",
      "84/84 [==============================] - 0s 776us/sample - loss: 0.0796 - acc: 0.9524\n",
      "Epoch 342/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0616 - acc: 0.9762\n",
      "Epoch 343/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0347 - acc: 0.9881\n",
      "Epoch 344/1000\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0497 - acc: 0.9881\n",
      "Epoch 345/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.0792 - acc: 0.9643\n",
      "Epoch 346/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0765 - acc: 0.9643\n",
      "Epoch 347/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0500 - acc: 0.9881\n",
      "Epoch 348/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.1013 - acc: 0.9524\n",
      "Epoch 349/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.0712 - acc: 0.9643\n",
      "Epoch 350/1000\n",
      "84/84 [==============================] - 0s 675us/sample - loss: 0.0574 - acc: 0.9881\n",
      "Epoch 351/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.1306 - acc: 0.9643\n",
      "Epoch 352/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0579 - acc: 0.9643\n",
      "Epoch 353/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0431 - acc: 0.9762\n",
      "Epoch 354/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.1263 - acc: 0.9643\n",
      "Epoch 355/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0476 - acc: 0.9762\n",
      "Epoch 356/1000\n",
      "84/84 [==============================] - 0s 687us/sample - loss: 0.0446 - acc: 0.9881\n",
      "Epoch 357/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0968 - acc: 0.9524\n",
      "Epoch 358/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0503 - acc: 0.9762\n",
      "Epoch 359/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0665 - acc: 0.9762\n",
      "Epoch 360/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0550 - acc: 0.9643\n",
      "Epoch 361/1000\n",
      "84/84 [==============================] - 0s 766us/sample - loss: 0.0626 - acc: 0.9643\n",
      "Epoch 362/1000\n",
      "84/84 [==============================] - 0s 732us/sample - loss: 0.0683 - acc: 0.9524\n",
      "Epoch 363/1000\n",
      "84/84 [==============================] - 0s 679us/sample - loss: 0.1165 - acc: 0.9524\n",
      "Epoch 364/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0537 - acc: 0.9762\n",
      "Epoch 365/1000\n",
      "84/84 [==============================] - 0s 792us/sample - loss: 0.0747 - acc: 0.9881\n",
      "Epoch 366/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0407 - acc: 1.0000\n",
      "Epoch 367/1000\n",
      "84/84 [==============================] - 0s 676us/sample - loss: 0.0481 - acc: 0.9881\n",
      "Epoch 368/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.0867 - acc: 0.9524\n",
      "Epoch 369/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.0708 - acc: 0.9762\n",
      "Epoch 370/1000\n",
      "84/84 [==============================] - 0s 685us/sample - loss: 0.0596 - acc: 0.9405\n",
      "Epoch 371/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.1282 - acc: 0.9643\n",
      "Epoch 372/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0795 - acc: 0.9405\n",
      "Epoch 373/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.0707 - acc: 0.9762\n",
      "Epoch 374/1000\n",
      "84/84 [==============================] - 0s 795us/sample - loss: 0.1256 - acc: 0.9762\n",
      "Epoch 375/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.1278 - acc: 0.9524\n",
      "Epoch 376/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0676 - acc: 0.9762\n",
      "Epoch 377/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0645 - acc: 0.9762\n",
      "Epoch 378/1000\n",
      "84/84 [==============================] - 0s 686us/sample - loss: 0.0583 - acc: 0.9762\n",
      "Epoch 379/1000\n",
      "84/84 [==============================] - 0s 776us/sample - loss: 0.0684 - acc: 0.9643\n",
      "Epoch 380/1000\n",
      "84/84 [==============================] - 0s 781us/sample - loss: 0.0677 - acc: 0.9762\n",
      "Epoch 381/1000\n",
      "84/84 [==============================] - 0s 811us/sample - loss: 0.0478 - acc: 0.9881\n",
      "Epoch 382/1000\n",
      "84/84 [==============================] - 0s 901us/sample - loss: 0.0751 - acc: 0.9524\n",
      "Epoch 383/1000\n",
      "84/84 [==============================] - 0s 906us/sample - loss: 0.0607 - acc: 0.9524\n",
      "Epoch 384/1000\n",
      "84/84 [==============================] - 0s 908us/sample - loss: 0.0799 - acc: 0.9524\n",
      "Epoch 385/1000\n",
      "84/84 [==============================] - 0s 863us/sample - loss: 0.0626 - acc: 0.9762\n",
      "Epoch 386/1000\n",
      "84/84 [==============================] - 0s 792us/sample - loss: 0.0432 - acc: 0.9881\n",
      "Epoch 387/1000\n",
      "84/84 [==============================] - 0s 816us/sample - loss: 0.1080 - acc: 0.9524\n",
      "Epoch 388/1000\n",
      "84/84 [==============================] - 0s 783us/sample - loss: 0.0548 - acc: 0.9762\n",
      "Epoch 389/1000\n",
      "84/84 [==============================] - 0s 803us/sample - loss: 0.0570 - acc: 0.9881\n",
      "Epoch 390/1000\n",
      "84/84 [==============================] - 0s 825us/sample - loss: 0.0282 - acc: 0.9881\n",
      "Epoch 391/1000\n",
      "84/84 [==============================] - 0s 864us/sample - loss: 0.1101 - acc: 0.9762\n",
      "Epoch 392/1000\n",
      "84/84 [==============================] - 0s 764us/sample - loss: 0.0953 - acc: 0.9762\n",
      "Epoch 393/1000\n",
      "84/84 [==============================] - 0s 785us/sample - loss: 0.0616 - acc: 0.9762\n",
      "Epoch 394/1000\n",
      "84/84 [==============================] - 0s 776us/sample - loss: 0.0613 - acc: 0.9643\n",
      "Epoch 395/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0859 - acc: 0.9524\n",
      "Epoch 396/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0622 - acc: 0.9762\n",
      "Epoch 397/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0671 - acc: 0.9762\n",
      "Epoch 398/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0485 - acc: 0.9643\n",
      "Epoch 399/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0595 - acc: 0.9643\n",
      "Epoch 400/1000\n",
      "84/84 [==============================] - 0s 701us/sample - loss: 0.0726 - acc: 0.9643\n",
      "Epoch 401/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0799 - acc: 0.9762\n",
      "Epoch 402/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.1431 - acc: 0.9524\n",
      "Epoch 403/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0660 - acc: 0.9762\n",
      "Epoch 404/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.0544 - acc: 0.9762\n",
      "Epoch 405/1000\n",
      "84/84 [==============================] - 0s 710us/sample - loss: 0.1087 - acc: 0.9405\n",
      "Epoch 406/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0577 - acc: 0.9643\n",
      "Epoch 407/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0546 - acc: 0.9643\n",
      "Epoch 408/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0776 - acc: 0.9643\n",
      "Epoch 409/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.0682 - acc: 0.9643\n",
      "Epoch 410/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0413 - acc: 0.9762\n",
      "Epoch 411/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0632 - acc: 0.9643\n",
      "Epoch 412/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.0496 - acc: 0.9881\n",
      "Epoch 413/1000\n",
      "84/84 [==============================] - 0s 771us/sample - loss: 0.0762 - acc: 0.9524\n",
      "Epoch 414/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0644 - acc: 0.9762\n",
      "Epoch 415/1000\n",
      "84/84 [==============================] - 0s 832us/sample - loss: 0.0596 - acc: 0.9762\n",
      "Epoch 416/1000\n",
      "84/84 [==============================] - 0s 772us/sample - loss: 0.1030 - acc: 0.9762\n",
      "Epoch 417/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0429 - acc: 0.9762\n",
      "Epoch 418/1000\n",
      "84/84 [==============================] - 0s 798us/sample - loss: 0.1455 - acc: 0.9643\n",
      "Epoch 419/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0682 - acc: 0.9643\n",
      "Epoch 420/1000\n",
      "84/84 [==============================] - 0s 684us/sample - loss: 0.0571 - acc: 0.9881\n",
      "Epoch 421/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0809 - acc: 0.9286\n",
      "Epoch 422/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0522 - acc: 0.9643\n",
      "Epoch 423/1000\n",
      "84/84 [==============================] - 0s 690us/sample - loss: 0.0652 - acc: 0.9643\n",
      "Epoch 424/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0482 - acc: 0.9881\n",
      "Epoch 425/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0458 - acc: 0.9762\n",
      "Epoch 426/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.1873 - acc: 0.9405\n",
      "Epoch 427/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.1107 - acc: 0.9405\n",
      "Epoch 428/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0582 - acc: 0.9881\n",
      "Epoch 429/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.0483 - acc: 0.9881\n",
      "Epoch 430/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0769 - acc: 0.9524\n",
      "Epoch 431/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.0750 - acc: 0.9762\n",
      "Epoch 432/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.0532 - acc: 0.9762\n",
      "Epoch 433/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.1405 - acc: 0.9524\n",
      "Epoch 434/1000\n",
      "84/84 [==============================] - 0s 790us/sample - loss: 0.0611 - acc: 0.9643\n",
      "Epoch 435/1000\n",
      "84/84 [==============================] - 0s 786us/sample - loss: 0.0544 - acc: 0.9881\n",
      "Epoch 436/1000\n",
      "84/84 [==============================] - 0s 771us/sample - loss: 0.0766 - acc: 0.9524\n",
      "Epoch 437/1000\n",
      "84/84 [==============================] - 0s 764us/sample - loss: 0.0553 - acc: 0.9762\n",
      "Epoch 438/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0629 - acc: 0.9524\n",
      "Epoch 439/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0782 - acc: 0.9762\n",
      "Epoch 440/1000\n",
      "84/84 [==============================] - 0s 710us/sample - loss: 0.0622 - acc: 0.9762\n",
      "Epoch 441/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0550 - acc: 0.9881\n",
      "Epoch 442/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0543 - acc: 0.9762\n",
      "Epoch 443/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0499 - acc: 0.9762\n",
      "Epoch 444/1000\n",
      "84/84 [==============================] - 0s 710us/sample - loss: 0.0968 - acc: 0.9524\n",
      "Epoch 445/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.0610 - acc: 0.9762\n",
      "Epoch 446/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0305 - acc: 0.9881\n",
      "Epoch 447/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0571 - acc: 0.9643\n",
      "Epoch 448/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0554 - acc: 0.9762\n",
      "Epoch 449/1000\n",
      "84/84 [==============================] - 0s 764us/sample - loss: 0.0935 - acc: 0.9643\n",
      "Epoch 450/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0455 - acc: 0.9762\n",
      "Epoch 451/1000\n",
      "84/84 [==============================] - 0s 769us/sample - loss: 0.0497 - acc: 0.9881\n",
      "Epoch 452/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0527 - acc: 0.9762\n",
      "Epoch 453/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0857 - acc: 0.9643\n",
      "Epoch 454/1000\n",
      "84/84 [==============================] - 0s 665us/sample - loss: 0.0576 - acc: 0.9405\n",
      "Epoch 455/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0535 - acc: 0.9881\n",
      "Epoch 456/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0744 - acc: 0.9762\n",
      "Epoch 457/1000\n",
      "84/84 [==============================] - 0s 691us/sample - loss: 0.0629 - acc: 0.9762\n",
      "Epoch 458/1000\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.0527 - acc: 0.9762\n",
      "Epoch 459/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.0346 - acc: 0.9881\n",
      "Epoch 460/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.0799 - acc: 0.9643\n",
      "Epoch 461/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0547 - acc: 0.9881\n",
      "Epoch 462/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0521 - acc: 0.9643\n",
      "Epoch 463/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0549 - acc: 0.9643\n",
      "Epoch 464/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0535 - acc: 0.9881\n",
      "Epoch 465/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0776 - acc: 0.9762\n",
      "Epoch 466/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.1438 - acc: 0.9524\n",
      "Epoch 467/1000\n",
      "84/84 [==============================] - 0s 814us/sample - loss: 0.0740 - acc: 0.9643\n",
      "Epoch 468/1000\n",
      "84/84 [==============================] - 0s 823us/sample - loss: 0.0773 - acc: 0.9524\n",
      "Epoch 469/1000\n",
      "84/84 [==============================] - 0s 811us/sample - loss: 0.0756 - acc: 0.9881\n",
      "Epoch 470/1000\n",
      "84/84 [==============================] - 0s 793us/sample - loss: 0.0548 - acc: 0.9881\n",
      "Epoch 471/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0423 - acc: 0.9643\n",
      "Epoch 472/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0509 - acc: 0.9881\n",
      "Epoch 473/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0730 - acc: 0.9643\n",
      "Epoch 474/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0487 - acc: 0.9762\n",
      "Epoch 475/1000\n",
      "84/84 [==============================] - 0s 865us/sample - loss: 0.0754 - acc: 0.9881\n",
      "Epoch 476/1000\n",
      "84/84 [==============================] - 0s 790us/sample - loss: 0.0977 - acc: 0.9405\n",
      "Epoch 477/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0471 - acc: 0.9643\n",
      "Epoch 478/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 764us/sample - loss: 0.0876 - acc: 0.9405\n",
      "Epoch 479/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0770 - acc: 0.9643\n",
      "Epoch 480/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0764 - acc: 0.9524\n",
      "Epoch 481/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0827 - acc: 0.9524\n",
      "Epoch 482/1000\n",
      "84/84 [==============================] - 0s 692us/sample - loss: 0.0836 - acc: 0.9643\n",
      "Epoch 483/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 484/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0805 - acc: 0.9524\n",
      "Epoch 485/1000\n",
      "84/84 [==============================] - 0s 839us/sample - loss: 0.0496 - acc: 0.9762\n",
      "Epoch 486/1000\n",
      "84/84 [==============================] - 0s 801us/sample - loss: 0.0607 - acc: 0.9762\n",
      "Epoch 487/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.0601 - acc: 0.9881\n",
      "Epoch 488/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0622 - acc: 0.9762\n",
      "Epoch 489/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0505 - acc: 0.9762\n",
      "Epoch 490/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0552 - acc: 0.9643\n",
      "Epoch 491/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.0882 - acc: 0.9524\n",
      "Epoch 492/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0609 - acc: 0.9643\n",
      "Epoch 493/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.1024 - acc: 0.9643\n",
      "Epoch 494/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0667 - acc: 0.9524\n",
      "Epoch 495/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0498 - acc: 0.9643\n",
      "Epoch 496/1000\n",
      "84/84 [==============================] - 0s 938us/sample - loss: 0.0511 - acc: 0.9762\n",
      "Epoch 497/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0882 - acc: 0.9643\n",
      "Epoch 498/1000\n",
      "84/84 [==============================] - 0s 944us/sample - loss: 0.0685 - acc: 0.9643\n",
      "Epoch 499/1000\n",
      "84/84 [==============================] - 0s 916us/sample - loss: 0.0532 - acc: 0.9762\n",
      "Epoch 500/1000\n",
      "84/84 [==============================] - 0s 884us/sample - loss: 0.0624 - acc: 0.9643\n",
      "Epoch 501/1000\n",
      "84/84 [==============================] - 0s 849us/sample - loss: 0.2052 - acc: 0.9524\n",
      "Epoch 502/1000\n",
      "84/84 [==============================] - 0s 797us/sample - loss: 0.0787 - acc: 0.9643\n",
      "Epoch 503/1000\n",
      "84/84 [==============================] - 0s 775us/sample - loss: 0.0460 - acc: 0.9881\n",
      "Epoch 504/1000\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0587 - acc: 0.9643\n",
      "Epoch 505/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.0716 - acc: 0.9762\n",
      "Epoch 506/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0497 - acc: 0.9762\n",
      "Epoch 507/1000\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.0606 - acc: 0.9643\n",
      "Epoch 508/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.0587 - acc: 0.9762\n",
      "Epoch 509/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0436 - acc: 0.9762\n",
      "Epoch 510/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0495 - acc: 0.9643\n",
      "Epoch 511/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0502 - acc: 0.9643\n",
      "Epoch 512/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0727 - acc: 0.9643\n",
      "Epoch 513/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.1454 - acc: 0.9524\n",
      "Epoch 514/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.0672 - acc: 0.9762\n",
      "Epoch 515/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0949 - acc: 0.9524\n",
      "Epoch 516/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0636 - acc: 0.9881\n",
      "Epoch 517/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0629 - acc: 0.9762\n",
      "Epoch 518/1000\n",
      "84/84 [==============================] - 0s 736us/sample - loss: 0.0640 - acc: 0.9762\n",
      "Epoch 519/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0580 - acc: 0.9643\n",
      "Epoch 520/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.0442 - acc: 0.9762\n",
      "Epoch 521/1000\n",
      "84/84 [==============================] - 0s 700us/sample - loss: 0.0458 - acc: 0.9762\n",
      "Epoch 522/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0599 - acc: 0.9524\n",
      "Epoch 523/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0382 - acc: 0.9762\n",
      "Epoch 524/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.1294 - acc: 0.9524\n",
      "Epoch 525/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0452 - acc: 0.9881\n",
      "Epoch 526/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0762 - acc: 0.9643\n",
      "Epoch 527/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0570 - acc: 0.9643\n",
      "Epoch 528/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0568 - acc: 0.9762\n",
      "Epoch 529/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0647 - acc: 0.9643\n",
      "Epoch 530/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.1309 - acc: 0.9405\n",
      "Epoch 531/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0448 - acc: 0.9762\n",
      "Epoch 532/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 533/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0524 - acc: 0.9643\n",
      "Epoch 534/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0700 - acc: 0.9881\n",
      "Epoch 535/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 536/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.1846 - acc: 0.9405\n",
      "Epoch 537/1000\n",
      "84/84 [==============================] - 0s 698us/sample - loss: 0.0526 - acc: 0.9762\n",
      "Epoch 538/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0485 - acc: 0.9762\n",
      "Epoch 539/1000\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.0692 - acc: 0.9524\n",
      "Epoch 540/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0581 - acc: 0.9881\n",
      "Epoch 541/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0488 - acc: 0.9762\n",
      "Epoch 542/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0380 - acc: 0.9881\n",
      "Epoch 543/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0673 - acc: 0.9643\n",
      "Epoch 544/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0677 - acc: 0.9762\n",
      "Epoch 545/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0578 - acc: 0.9643\n",
      "Epoch 546/1000\n",
      "84/84 [==============================] - 0s 775us/sample - loss: 0.5817 - acc: 0.9286\n",
      "Epoch 547/1000\n",
      "84/84 [==============================] - 0s 751us/sample - loss: 0.0767 - acc: 0.9762\n",
      "Epoch 548/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.0671 - acc: 0.9643\n",
      "Epoch 549/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.0626 - acc: 0.9643\n",
      "Epoch 550/1000\n",
      "84/84 [==============================] - 0s 766us/sample - loss: 0.0585 - acc: 0.9881\n",
      "Epoch 551/1000\n",
      "84/84 [==============================] - 0s 780us/sample - loss: 0.0455 - acc: 0.9643\n",
      "Epoch 552/1000\n",
      "84/84 [==============================] - 0s 789us/sample - loss: 0.0774 - acc: 0.9405\n",
      "Epoch 553/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0477 - acc: 0.9762\n",
      "Epoch 554/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0692 - acc: 0.9524\n",
      "Epoch 555/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0545 - acc: 0.9881\n",
      "Epoch 556/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.0444 - acc: 0.9881\n",
      "Epoch 557/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0361 - acc: 0.9881\n",
      "Epoch 558/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0662 - acc: 0.9643\n",
      "Epoch 559/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 815us/sample - loss: 0.0607 - acc: 0.9762\n",
      "Epoch 560/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0962 - acc: 0.9762\n",
      "Epoch 561/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0562 - acc: 0.9762\n",
      "Epoch 562/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0886 - acc: 0.9643\n",
      "Epoch 563/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0974 - acc: 0.9405\n",
      "Epoch 564/1000\n",
      "84/84 [==============================] - 0s 768us/sample - loss: 0.0495 - acc: 0.9762\n",
      "Epoch 565/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0435 - acc: 0.9762\n",
      "Epoch 566/1000\n",
      "84/84 [==============================] - 0s 771us/sample - loss: 0.0438 - acc: 0.9881\n",
      "Epoch 567/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0794 - acc: 0.9524\n",
      "Epoch 568/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.1713 - acc: 0.9286\n",
      "Epoch 569/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0519 - acc: 0.9643\n",
      "Epoch 570/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0565 - acc: 0.9762\n",
      "Epoch 571/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0601 - acc: 0.9643\n",
      "Epoch 572/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.1118 - acc: 0.9405\n",
      "Epoch 573/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0677 - acc: 0.9524\n",
      "Epoch 574/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0639 - acc: 0.9762\n",
      "Epoch 575/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0826 - acc: 0.9524\n",
      "Epoch 576/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0430 - acc: 0.9881\n",
      "Epoch 577/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0443 - acc: 0.9881\n",
      "Epoch 578/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0877 - acc: 0.9762\n",
      "Epoch 579/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0800 - acc: 0.9643\n",
      "Epoch 580/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0422 - acc: 0.9881\n",
      "Epoch 581/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.3204 - acc: 0.9167\n",
      "Epoch 582/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0721 - acc: 0.9762\n",
      "Epoch 583/1000\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0822 - acc: 0.9762\n",
      "Epoch 584/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0670 - acc: 0.9643\n",
      "Epoch 585/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0543 - acc: 0.9643\n",
      "Epoch 586/1000\n",
      "84/84 [==============================] - 0s 865us/sample - loss: 0.0496 - acc: 0.9762\n",
      "Epoch 587/1000\n",
      "84/84 [==============================] - 0s 854us/sample - loss: 0.0436 - acc: 0.9762\n",
      "Epoch 588/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.1189 - acc: 0.9524\n",
      "Epoch 589/1000\n",
      "84/84 [==============================] - 0s 761us/sample - loss: 0.0522 - acc: 0.9881\n",
      "Epoch 590/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0524 - acc: 0.9762\n",
      "Epoch 591/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0505 - acc: 0.9881\n",
      "Epoch 592/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0623 - acc: 0.9524\n",
      "Epoch 593/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0525 - acc: 0.9762\n",
      "Epoch 594/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0520 - acc: 0.9643\n",
      "Epoch 595/1000\n",
      "84/84 [==============================] - 0s 698us/sample - loss: 0.0560 - acc: 0.9762\n",
      "Epoch 596/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0537 - acc: 0.9643\n",
      "Epoch 597/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0696 - acc: 0.9643\n",
      "Epoch 598/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0972 - acc: 0.9405\n",
      "Epoch 599/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0562 - acc: 0.9643\n",
      "Epoch 600/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0505 - acc: 0.9643\n",
      "Epoch 601/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0651 - acc: 0.9762\n",
      "Epoch 602/1000\n",
      "84/84 [==============================] - 0s 688us/sample - loss: 0.0874 - acc: 0.9405\n",
      "Epoch 603/1000\n",
      "84/84 [==============================] - 0s 694us/sample - loss: 0.0506 - acc: 0.9881\n",
      "Epoch 604/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0609 - acc: 0.9524\n",
      "Epoch 605/1000\n",
      "84/84 [==============================] - 0s 842us/sample - loss: 0.0394 - acc: 0.9762\n",
      "Epoch 606/1000\n",
      "84/84 [==============================] - 0s 797us/sample - loss: 0.0697 - acc: 0.9643\n",
      "Epoch 607/1000\n",
      "84/84 [==============================] - 0s 794us/sample - loss: 0.0744 - acc: 0.9643\n",
      "Epoch 608/1000\n",
      "84/84 [==============================] - 0s 692us/sample - loss: 0.0599 - acc: 0.9762\n",
      "Epoch 609/1000\n",
      "84/84 [==============================] - 0s 779us/sample - loss: 0.0464 - acc: 0.9762\n",
      "Epoch 610/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0726 - acc: 0.9762\n",
      "Epoch 611/1000\n",
      "84/84 [==============================] - 0s 823us/sample - loss: 0.0368 - acc: 0.9762\n",
      "Epoch 612/1000\n",
      "84/84 [==============================] - 0s 844us/sample - loss: 0.0439 - acc: 0.9762\n",
      "Epoch 613/1000\n",
      "84/84 [==============================] - 0s 812us/sample - loss: 0.0388 - acc: 0.9762\n",
      "Epoch 614/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.1037 - acc: 0.9524\n",
      "Epoch 615/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.1100 - acc: 0.9405\n",
      "Epoch 616/1000\n",
      "84/84 [==============================] - 0s 666us/sample - loss: 0.0686 - acc: 0.9762\n",
      "Epoch 617/1000\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.0470 - acc: 0.9881\n",
      "Epoch 618/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0557 - acc: 0.9762\n",
      "Epoch 619/1000\n",
      "84/84 [==============================] - 0s 794us/sample - loss: 0.0247 - acc: 1.0000\n",
      "Epoch 620/1000\n",
      "84/84 [==============================] - 0s 794us/sample - loss: 0.2825 - acc: 0.9524\n",
      "Epoch 621/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0669 - acc: 0.9643\n",
      "Epoch 622/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0546 - acc: 0.9762\n",
      "Epoch 623/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0951 - acc: 0.9524\n",
      "Epoch 624/1000\n",
      "84/84 [==============================] - 0s 753us/sample - loss: 0.0523 - acc: 0.9643\n",
      "Epoch 625/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0526 - acc: 0.9762\n",
      "Epoch 626/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0559 - acc: 0.9762\n",
      "Epoch 627/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0644 - acc: 0.9643\n",
      "Epoch 628/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0658 - acc: 0.9643\n",
      "Epoch 629/1000\n",
      "84/84 [==============================] - 0s 760us/sample - loss: 0.0507 - acc: 0.9762\n",
      "Epoch 630/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0475 - acc: 0.9762\n",
      "Epoch 631/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0531 - acc: 0.9762\n",
      "Epoch 632/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0705 - acc: 0.9524\n",
      "Epoch 633/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0671 - acc: 0.9643\n",
      "Epoch 634/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.1014 - acc: 0.9405\n",
      "Epoch 635/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.0595 - acc: 0.9762\n",
      "Epoch 636/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0641 - acc: 0.9762\n",
      "Epoch 637/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0577 - acc: 0.9881\n",
      "Epoch 638/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0503 - acc: 0.9762\n",
      "Epoch 639/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0522 - acc: 0.9762\n",
      "Epoch 640/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 753us/sample - loss: 0.4930 - acc: 0.9167\n",
      "Epoch 641/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0690 - acc: 0.9643\n",
      "Epoch 642/1000\n",
      "84/84 [==============================] - 0s 732us/sample - loss: 0.6668 - acc: 0.9524\n",
      "Epoch 643/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.1844 - acc: 0.9286\n",
      "Epoch 644/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0948 - acc: 0.9762\n",
      "Epoch 645/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.0761 - acc: 0.9643\n",
      "Epoch 646/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0794 - acc: 0.9524\n",
      "Epoch 647/1000\n",
      "84/84 [==============================] - 0s 816us/sample - loss: 0.0485 - acc: 0.9762\n",
      "Epoch 648/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0652 - acc: 0.9762\n",
      "Epoch 649/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0659 - acc: 0.9405\n",
      "Epoch 650/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0690 - acc: 0.9762\n",
      "Epoch 651/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0751 - acc: 0.9762\n",
      "Epoch 652/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0732 - acc: 0.9524\n",
      "Epoch 653/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0587 - acc: 0.9881\n",
      "Epoch 654/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0625 - acc: 0.9524\n",
      "Epoch 655/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0617 - acc: 0.9643\n",
      "Epoch 656/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0970 - acc: 0.9762\n",
      "Epoch 657/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0500 - acc: 0.9881\n",
      "Epoch 658/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0616 - acc: 0.9762\n",
      "Epoch 659/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0531 - acc: 0.9881\n",
      "Epoch 660/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0761 - acc: 0.9524\n",
      "Epoch 661/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0673 - acc: 0.9762\n",
      "Epoch 662/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.0323 - acc: 0.9881\n",
      "Epoch 663/1000\n",
      "84/84 [==============================] - 0s 690us/sample - loss: 0.0918 - acc: 0.9643\n",
      "Epoch 664/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0317 - acc: 0.9881\n",
      "Epoch 665/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0645 - acc: 0.9643\n",
      "Epoch 666/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.0575 - acc: 0.9762\n",
      "Epoch 667/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.1482 - acc: 0.9405\n",
      "Epoch 668/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0677 - acc: 0.9524\n",
      "Epoch 669/1000\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.0335 - acc: 0.9762\n",
      "Epoch 670/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0602 - acc: 0.9881\n",
      "Epoch 671/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.1158 - acc: 0.9643\n",
      "Epoch 672/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.0952 - acc: 0.9643\n",
      "Epoch 673/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0862 - acc: 0.9643\n",
      "Epoch 674/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0496 - acc: 0.9762\n",
      "Epoch 675/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0405 - acc: 0.9881\n",
      "Epoch 676/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0411 - acc: 0.9762\n",
      "Epoch 677/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0350 - acc: 0.9881\n",
      "Epoch 678/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0692 - acc: 0.9643\n",
      "Epoch 679/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0740 - acc: 0.9643\n",
      "Epoch 680/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0432 - acc: 0.9881\n",
      "Epoch 681/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0740 - acc: 0.9762\n",
      "Epoch 682/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0705 - acc: 0.9643\n",
      "Epoch 683/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0780 - acc: 0.9762\n",
      "Epoch 684/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.0621 - acc: 0.9881\n",
      "Epoch 685/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0620 - acc: 0.9762\n",
      "Epoch 686/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0506 - acc: 0.9762\n",
      "Epoch 687/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0815 - acc: 0.9524\n",
      "Epoch 688/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.0551 - acc: 0.9762\n",
      "Epoch 689/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0617 - acc: 0.9762\n",
      "Epoch 690/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0537 - acc: 0.9643\n",
      "Epoch 691/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.1094 - acc: 0.9405\n",
      "Epoch 692/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0349 - acc: 0.9881\n",
      "Epoch 693/1000\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0971 - acc: 0.9643\n",
      "Epoch 694/1000\n",
      "84/84 [==============================] - 0s 751us/sample - loss: 0.0666 - acc: 0.9524\n",
      "Epoch 695/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0888 - acc: 0.9643\n",
      "Epoch 696/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0670 - acc: 0.9762\n",
      "Epoch 697/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.1795 - acc: 0.9643\n",
      "Epoch 698/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0513 - acc: 0.9643\n",
      "Epoch 699/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0657 - acc: 0.9643\n",
      "Epoch 700/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0993 - acc: 0.9643\n",
      "Epoch 701/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0749 - acc: 0.9405\n",
      "Epoch 702/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0571 - acc: 0.9643\n",
      "Epoch 703/1000\n",
      "84/84 [==============================] - 0s 760us/sample - loss: 0.0362 - acc: 0.9881\n",
      "Epoch 704/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 705/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0789 - acc: 0.9643\n",
      "Epoch 706/1000\n",
      "84/84 [==============================] - 0s 772us/sample - loss: 0.0707 - acc: 0.9762\n",
      "Epoch 707/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0525 - acc: 0.9762\n",
      "Epoch 708/1000\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.0503 - acc: 0.9881\n",
      "Epoch 709/1000\n",
      "84/84 [==============================] - 0s 780us/sample - loss: 0.0767 - acc: 0.9643\n",
      "Epoch 710/1000\n",
      "84/84 [==============================] - 0s 753us/sample - loss: 0.0722 - acc: 0.9762\n",
      "Epoch 711/1000\n",
      "84/84 [==============================] - 0s 811us/sample - loss: 0.1131 - acc: 0.9405\n",
      "Epoch 712/1000\n",
      "84/84 [==============================] - 0s 841us/sample - loss: 0.0582 - acc: 0.9643\n",
      "Epoch 713/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0484 - acc: 0.9762\n",
      "Epoch 714/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0946 - acc: 0.9405\n",
      "Epoch 715/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.1424 - acc: 0.9524\n",
      "Epoch 716/1000\n",
      "84/84 [==============================] - 0s 751us/sample - loss: 0.0771 - acc: 0.9762\n",
      "Epoch 717/1000\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.0610 - acc: 0.9762\n",
      "Epoch 718/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0828 - acc: 0.9643\n",
      "Epoch 719/1000\n",
      "84/84 [==============================] - 0s 742us/sample - loss: 0.0941 - acc: 0.9524\n",
      "Epoch 720/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.0599 - acc: 0.9762\n",
      "Epoch 721/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0434 - acc: 0.9762\n",
      "Epoch 722/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0385 - acc: 0.9762\n",
      "Epoch 723/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0453 - acc: 0.9762\n",
      "Epoch 724/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0824 - acc: 0.9643\n",
      "Epoch 725/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0873 - acc: 0.9643\n",
      "Epoch 726/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0352 - acc: 0.9881\n",
      "Epoch 727/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0847 - acc: 0.9643\n",
      "Epoch 728/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0508 - acc: 0.9762\n",
      "Epoch 729/1000\n",
      "84/84 [==============================] - 0s 771us/sample - loss: 0.0761 - acc: 0.9643\n",
      "Epoch 730/1000\n",
      "84/84 [==============================] - 0s 905us/sample - loss: 0.0549 - acc: 0.9643\n",
      "Epoch 731/1000\n",
      "84/84 [==============================] - 0s 783us/sample - loss: 0.0422 - acc: 0.9643\n",
      "Epoch 732/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.1703 - acc: 0.9405\n",
      "Epoch 733/1000\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.1454 - acc: 0.9762\n",
      "Epoch 734/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0985 - acc: 0.9405\n",
      "Epoch 735/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0601 - acc: 0.9762\n",
      "Epoch 736/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0748 - acc: 0.9643\n",
      "Epoch 737/1000\n",
      "84/84 [==============================] - 0s 822us/sample - loss: 0.0651 - acc: 0.9762\n",
      "Epoch 738/1000\n",
      "84/84 [==============================] - 0s 784us/sample - loss: 0.0401 - acc: 0.9881\n",
      "Epoch 739/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0391 - acc: 0.9881\n",
      "Epoch 740/1000\n",
      "84/84 [==============================] - 0s 732us/sample - loss: 0.0387 - acc: 0.9762\n",
      "Epoch 741/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.0723 - acc: 0.9643\n",
      "Epoch 742/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0468 - acc: 0.9643\n",
      "Epoch 743/1000\n",
      "84/84 [==============================] - 0s 755us/sample - loss: 0.0389 - acc: 0.9881\n",
      "Epoch 744/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0756 - acc: 0.9643\n",
      "Epoch 745/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0453 - acc: 0.9524\n",
      "Epoch 746/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0424 - acc: 0.9881\n",
      "Epoch 747/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0804 - acc: 0.9762\n",
      "Epoch 748/1000\n",
      "84/84 [==============================] - 0s 857us/sample - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 749/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0562 - acc: 0.9881\n",
      "Epoch 750/1000\n",
      "84/84 [==============================] - 0s 762us/sample - loss: 0.1184 - acc: 0.9405\n",
      "Epoch 751/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.0873 - acc: 0.9524\n",
      "Epoch 752/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0601 - acc: 0.9643\n",
      "Epoch 753/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0350 - acc: 0.9881\n",
      "Epoch 754/1000\n",
      "84/84 [==============================] - 0s 716us/sample - loss: 0.0560 - acc: 0.9762\n",
      "Epoch 755/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0675 - acc: 0.9524\n",
      "Epoch 756/1000\n",
      "84/84 [==============================] - 0s 732us/sample - loss: 0.0560 - acc: 0.9643\n",
      "Epoch 757/1000\n",
      "84/84 [==============================] - 0s 758us/sample - loss: 0.0476 - acc: 0.9643\n",
      "Epoch 758/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0299 - acc: 0.9881\n",
      "Epoch 759/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0897 - acc: 0.9762\n",
      "Epoch 760/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0639 - acc: 0.9762\n",
      "Epoch 761/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0840 - acc: 0.9643\n",
      "Epoch 762/1000\n",
      "84/84 [==============================] - 0s 773us/sample - loss: 0.0395 - acc: 0.9762\n",
      "Epoch 763/1000\n",
      "84/84 [==============================] - 0s 742us/sample - loss: 0.2336 - acc: 0.9167\n",
      "Epoch 764/1000\n",
      "84/84 [==============================] - 0s 769us/sample - loss: 0.0738 - acc: 0.9762\n",
      "Epoch 765/1000\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0622 - acc: 0.9762\n",
      "Epoch 766/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0449 - acc: 0.9643\n",
      "Epoch 767/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0443 - acc: 0.9881\n",
      "Epoch 768/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0518 - acc: 0.9762\n",
      "Epoch 769/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0566 - acc: 0.9643\n",
      "Epoch 770/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0490 - acc: 0.9881\n",
      "Epoch 771/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0583 - acc: 0.9762\n",
      "Epoch 772/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0455 - acc: 0.9643\n",
      "Epoch 773/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0502 - acc: 0.9881\n",
      "Epoch 774/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0712 - acc: 0.9643\n",
      "Epoch 775/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0598 - acc: 0.9762\n",
      "Epoch 776/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0360 - acc: 0.9762\n",
      "Epoch 777/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0444 - acc: 0.9643\n",
      "Epoch 778/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0776 - acc: 0.9524\n",
      "Epoch 779/1000\n",
      "84/84 [==============================] - 0s 779us/sample - loss: 0.0761 - acc: 0.9524\n",
      "Epoch 780/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 781/1000\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0996 - acc: 0.9524\n",
      "Epoch 782/1000\n",
      "84/84 [==============================] - 0s 751us/sample - loss: 0.0439 - acc: 0.9762\n",
      "Epoch 783/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0405 - acc: 0.9762\n",
      "Epoch 784/1000\n",
      "84/84 [==============================] - 0s 732us/sample - loss: 0.0479 - acc: 0.9881\n",
      "Epoch 785/1000\n",
      "84/84 [==============================] - 0s 702us/sample - loss: 0.0807 - acc: 0.9643\n",
      "Epoch 786/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0449 - acc: 0.9643\n",
      "Epoch 787/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0729 - acc: 0.9762\n",
      "Epoch 788/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0804 - acc: 0.9762\n",
      "Epoch 789/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0887 - acc: 0.9405\n",
      "Epoch 790/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0447 - acc: 0.9762\n",
      "Epoch 791/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0478 - acc: 0.9762\n",
      "Epoch 792/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0631 - acc: 0.9643\n",
      "Epoch 793/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0458 - acc: 0.9643\n",
      "Epoch 794/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0930 - acc: 0.9762\n",
      "Epoch 795/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.0643 - acc: 0.9762\n",
      "Epoch 796/1000\n",
      "84/84 [==============================] - 0s 710us/sample - loss: 0.0478 - acc: 0.9881\n",
      "Epoch 797/1000\n",
      "84/84 [==============================] - 0s 757us/sample - loss: 0.0648 - acc: 0.9524\n",
      "Epoch 798/1000\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0423 - acc: 1.0000\n",
      "Epoch 799/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0609 - acc: 0.9762\n",
      "Epoch 800/1000\n",
      "84/84 [==============================] - 0s 697us/sample - loss: 0.0593 - acc: 0.9524\n",
      "Epoch 801/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0389 - acc: 0.9762\n",
      "Epoch 802/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0638 - acc: 0.9524\n",
      "Epoch 803/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0496 - acc: 0.9762\n",
      "Epoch 804/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0454 - acc: 0.9881\n",
      "Epoch 805/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.3463 - acc: 0.9405\n",
      "Epoch 806/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0625 - acc: 0.9643\n",
      "Epoch 807/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0549 - acc: 0.9643\n",
      "Epoch 808/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0552 - acc: 0.9762\n",
      "Epoch 809/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0455 - acc: 0.9762\n",
      "Epoch 810/1000\n",
      "84/84 [==============================] - 0s 775us/sample - loss: 0.0450 - acc: 0.9643\n",
      "Epoch 811/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0526 - acc: 0.9762\n",
      "Epoch 812/1000\n",
      "84/84 [==============================] - 0s 693us/sample - loss: 0.0492 - acc: 0.9643\n",
      "Epoch 813/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0405 - acc: 0.9762\n",
      "Epoch 814/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0784 - acc: 0.9524\n",
      "Epoch 815/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.1024 - acc: 0.9524\n",
      "Epoch 816/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.0511 - acc: 0.9762\n",
      "Epoch 817/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.0496 - acc: 0.9762\n",
      "Epoch 818/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0604 - acc: 0.9762\n",
      "Epoch 819/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0664 - acc: 0.9643\n",
      "Epoch 820/1000\n",
      "84/84 [==============================] - 0s 757us/sample - loss: 0.0618 - acc: 0.9762\n",
      "Epoch 821/1000\n",
      "84/84 [==============================] - 0s 764us/sample - loss: 0.0433 - acc: 0.9762\n",
      "Epoch 822/1000\n",
      "84/84 [==============================] - 0s 736us/sample - loss: 0.0472 - acc: 0.9643\n",
      "Epoch 823/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0579 - acc: 0.9643\n",
      "Epoch 824/1000\n",
      "84/84 [==============================] - 0s 698us/sample - loss: 0.0789 - acc: 0.9524\n",
      "Epoch 825/1000\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0728 - acc: 0.9881\n",
      "Epoch 826/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0538 - acc: 0.9762\n",
      "Epoch 827/1000\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0645 - acc: 0.9643\n",
      "Epoch 828/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0600 - acc: 0.9762\n",
      "Epoch 829/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0489 - acc: 0.9643\n",
      "Epoch 830/1000\n",
      "84/84 [==============================] - 0s 829us/sample - loss: 0.0531 - acc: 0.9762\n",
      "Epoch 831/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0774 - acc: 0.9524\n",
      "Epoch 832/1000\n",
      "84/84 [==============================] - 0s 789us/sample - loss: 0.0518 - acc: 0.9643\n",
      "Epoch 833/1000\n",
      "84/84 [==============================] - 0s 779us/sample - loss: 0.0575 - acc: 0.9643\n",
      "Epoch 834/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0566 - acc: 0.9881\n",
      "Epoch 835/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0364 - acc: 0.9881\n",
      "Epoch 836/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0251 - acc: 0.9881\n",
      "Epoch 837/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.1001 - acc: 0.9405\n",
      "Epoch 838/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.1488 - acc: 0.9643\n",
      "Epoch 839/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.2660 - acc: 0.9286\n",
      "Epoch 840/1000\n",
      "84/84 [==============================] - 0s 703us/sample - loss: 0.0584 - acc: 0.9762\n",
      "Epoch 841/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0625 - acc: 0.9762\n",
      "Epoch 842/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0499 - acc: 0.9762\n",
      "Epoch 843/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0439 - acc: 0.9881\n",
      "Epoch 844/1000\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.0577 - acc: 0.9762\n",
      "Epoch 845/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0533 - acc: 0.9881\n",
      "Epoch 846/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0546 - acc: 0.9643\n",
      "Epoch 847/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0559 - acc: 0.9643\n",
      "Epoch 848/1000\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0581 - acc: 0.9524\n",
      "Epoch 849/1000\n",
      "84/84 [==============================] - 0s 757us/sample - loss: 0.0896 - acc: 0.9643\n",
      "Epoch 850/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0381 - acc: 0.9881\n",
      "Epoch 851/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0699 - acc: 0.9524\n",
      "Epoch 852/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0392 - acc: 0.9881\n",
      "Epoch 853/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0820 - acc: 0.9643\n",
      "Epoch 854/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0530 - acc: 0.9643\n",
      "Epoch 855/1000\n",
      "84/84 [==============================] - 0s 696us/sample - loss: 0.0577 - acc: 0.9524\n",
      "Epoch 856/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0554 - acc: 0.9881\n",
      "Epoch 857/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0480 - acc: 0.9762\n",
      "Epoch 858/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0693 - acc: 0.9643\n",
      "Epoch 859/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0366 - acc: 0.9881\n",
      "Epoch 860/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0496 - acc: 0.9524\n",
      "Epoch 861/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0465 - acc: 0.9762\n",
      "Epoch 862/1000\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.0746 - acc: 0.9524\n",
      "Epoch 863/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.1174 - acc: 0.9524\n",
      "Epoch 864/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0535 - acc: 0.9643\n",
      "Epoch 865/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0557 - acc: 0.9762\n",
      "Epoch 866/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0547 - acc: 0.9643\n",
      "Epoch 867/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0581 - acc: 0.9643\n",
      "Epoch 868/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0381 - acc: 0.9881\n",
      "Epoch 869/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0611 - acc: 0.9762\n",
      "Epoch 870/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0542 - acc: 0.9643\n",
      "Epoch 871/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0543 - acc: 0.9881\n",
      "Epoch 872/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0395 - acc: 0.9881\n",
      "Epoch 873/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0563 - acc: 0.9881\n",
      "Epoch 874/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0632 - acc: 0.9762\n",
      "Epoch 875/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0873 - acc: 0.9643\n",
      "Epoch 876/1000\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0670 - acc: 0.9524\n",
      "Epoch 877/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0854 - acc: 0.9762\n",
      "Epoch 878/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0468 - acc: 0.9881\n",
      "Epoch 879/1000\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0731 - acc: 0.9524\n",
      "Epoch 880/1000\n",
      "84/84 [==============================] - 0s 782us/sample - loss: 0.0484 - acc: 0.9762\n",
      "Epoch 881/1000\n",
      "84/84 [==============================] - 0s 753us/sample - loss: 0.0598 - acc: 0.9762\n",
      "Epoch 882/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0855 - acc: 0.9643\n",
      "Epoch 883/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0482 - acc: 0.9762\n",
      "Epoch 884/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0666 - acc: 0.9643\n",
      "Epoch 885/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0637 - acc: 0.9762\n",
      "Epoch 886/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0649 - acc: 0.9524\n",
      "Epoch 887/1000\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0458 - acc: 0.9881\n",
      "Epoch 888/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0481 - acc: 0.9762\n",
      "Epoch 889/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0441 - acc: 0.9762\n",
      "Epoch 890/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0688 - acc: 0.9762\n",
      "Epoch 891/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0800 - acc: 0.9762\n",
      "Epoch 892/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0516 - acc: 0.9762\n",
      "Epoch 893/1000\n",
      "84/84 [==============================] - 0s 733us/sample - loss: 0.0595 - acc: 0.9762\n",
      "Epoch 894/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0498 - acc: 0.9762\n",
      "Epoch 895/1000\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0508 - acc: 0.9762\n",
      "Epoch 896/1000\n",
      "84/84 [==============================] - 0s 780us/sample - loss: 0.0903 - acc: 0.9881\n",
      "Epoch 897/1000\n",
      "84/84 [==============================] - 0s 708us/sample - loss: 0.0709 - acc: 0.9643\n",
      "Epoch 898/1000\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0548 - acc: 0.9762\n",
      "Epoch 899/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0739 - acc: 0.9762\n",
      "Epoch 900/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0541 - acc: 0.9762\n",
      "Epoch 901/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0961 - acc: 0.9643\n",
      "Epoch 902/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0504 - acc: 0.9762\n",
      "Epoch 903/1000\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0358 - acc: 0.9881\n",
      "Epoch 904/1000\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0443 - acc: 0.9881\n",
      "Epoch 905/1000\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0518 - acc: 0.9762\n",
      "Epoch 906/1000\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.0466 - acc: 0.9762\n",
      "Epoch 907/1000\n",
      "84/84 [==============================] - 0s 782us/sample - loss: 0.0455 - acc: 0.9762\n",
      "Epoch 908/1000\n",
      "84/84 [==============================] - 0s 714us/sample - loss: 0.0690 - acc: 0.9762\n",
      "Epoch 909/1000\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.0848 - acc: 0.9762\n",
      "Epoch 910/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.1065 - acc: 0.9405\n",
      "Epoch 911/1000\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0602 - acc: 0.9881\n",
      "Epoch 912/1000\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0476 - acc: 0.9762\n",
      "Epoch 913/1000\n",
      "84/84 [==============================] - 0s 759us/sample - loss: 0.0585 - acc: 0.9643\n",
      "Epoch 914/1000\n",
      "84/84 [==============================] - 0s 707us/sample - loss: 0.0444 - acc: 0.9643\n",
      "Epoch 915/1000\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.0744 - acc: 0.9643\n",
      "Epoch 916/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0507 - acc: 0.9881\n",
      "Epoch 917/1000\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0538 - acc: 0.9643\n",
      "Epoch 918/1000\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.0398 - acc: 0.9881\n",
      "Epoch 919/1000\n",
      "84/84 [==============================] - 0s 736us/sample - loss: 0.0726 - acc: 0.9524\n",
      "Epoch 920/1000\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.2022 - acc: 0.9405\n",
      "Epoch 921/1000\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.0796 - acc: 0.9524\n",
      "Epoch 922/1000\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0504 - acc: 0.9881\n",
      "Epoch 923/1000\n",
      "84/84 [==============================] - 0s 779us/sample - loss: 0.0420 - acc: 0.9762\n",
      "Epoch 924/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0718 - acc: 0.9643\n",
      "Epoch 925/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0679 - acc: 0.9762\n",
      "Epoch 926/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0710 - acc: 0.9643\n",
      "Epoch 927/1000\n",
      "84/84 [==============================] - 0s 813us/sample - loss: 0.0668 - acc: 0.9643\n",
      "Epoch 928/1000\n",
      "84/84 [==============================] - 0s 794us/sample - loss: 0.0540 - acc: 0.9762\n",
      "Epoch 929/1000\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0667 - acc: 0.9643\n",
      "Epoch 930/1000\n",
      "84/84 [==============================] - 0s 736us/sample - loss: 0.0492 - acc: 0.9643\n",
      "Epoch 931/1000\n",
      "84/84 [==============================] - 0s 793us/sample - loss: 0.0595 - acc: 0.9762\n",
      "Epoch 932/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0643 - acc: 0.9524\n",
      "Epoch 933/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.0505 - acc: 0.9762\n",
      "Epoch 934/1000\n",
      "84/84 [==============================] - 0s 680us/sample - loss: 0.0542 - acc: 0.9643\n",
      "Epoch 935/1000\n",
      "84/84 [==============================] - 0s 742us/sample - loss: 0.0616 - acc: 0.9643\n",
      "Epoch 936/1000\n",
      "84/84 [==============================] - 0s 805us/sample - loss: 0.0435 - acc: 0.9762\n",
      "Epoch 937/1000\n",
      "84/84 [==============================] - 0s 750us/sample - loss: 0.0591 - acc: 0.9643\n",
      "Epoch 938/1000\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.0652 - acc: 0.9643\n",
      "Epoch 939/1000\n",
      "84/84 [==============================] - 0s 698us/sample - loss: 0.0686 - acc: 0.9762\n",
      "Epoch 940/1000\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.0545 - acc: 0.9643\n",
      "Epoch 941/1000\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.0540 - acc: 0.9762\n",
      "Epoch 942/1000\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0704 - acc: 0.9762\n",
      "Epoch 943/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.1347 - acc: 0.9524\n",
      "Epoch 944/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0859 - acc: 0.9524\n",
      "Epoch 945/1000\n",
      "84/84 [==============================] - 0s 787us/sample - loss: 0.0644 - acc: 0.9524\n",
      "Epoch 946/1000\n",
      "84/84 [==============================] - 0s 778us/sample - loss: 0.0573 - acc: 0.9643\n",
      "Epoch 947/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0921 - acc: 0.9524\n",
      "Epoch 948/1000\n",
      "84/84 [==============================] - 0s 781us/sample - loss: 0.0634 - acc: 0.9405\n",
      "Epoch 949/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0709 - acc: 0.9524\n",
      "Epoch 950/1000\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0528 - acc: 0.9643\n",
      "Epoch 951/1000\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.0467 - acc: 0.9762\n",
      "Epoch 952/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0442 - acc: 0.9881\n",
      "Epoch 953/1000\n",
      "84/84 [==============================] - 0s 760us/sample - loss: 0.0961 - acc: 0.9643\n",
      "Epoch 954/1000\n",
      "84/84 [==============================] - 0s 762us/sample - loss: 0.0492 - acc: 0.9762\n",
      "Epoch 955/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0704 - acc: 0.9643\n",
      "Epoch 956/1000\n",
      "84/84 [==============================] - 0s 749us/sample - loss: 0.0645 - acc: 0.9762\n",
      "Epoch 957/1000\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0910 - acc: 0.9643\n",
      "Epoch 958/1000\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.0584 - acc: 0.9762\n",
      "Epoch 959/1000\n",
      "84/84 [==============================] - 0s 709us/sample - loss: 0.0465 - acc: 0.9643\n",
      "Epoch 960/1000\n",
      "84/84 [==============================] - 0s 728us/sample - loss: 0.2811 - acc: 0.9405\n",
      "Epoch 961/1000\n",
      "84/84 [==============================] - 0s 710us/sample - loss: 0.0564 - acc: 0.9643\n",
      "Epoch 962/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0503 - acc: 0.9881\n",
      "Epoch 963/1000\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0516 - acc: 0.9881\n",
      "Epoch 964/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 725us/sample - loss: 0.0551 - acc: 0.9762\n",
      "Epoch 965/1000\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0797 - acc: 0.9643\n",
      "Epoch 966/1000\n",
      "84/84 [==============================] - 0s 726us/sample - loss: 0.0472 - acc: 0.9643\n",
      "Epoch 967/1000\n",
      "84/84 [==============================] - 0s 741us/sample - loss: 0.0593 - acc: 0.9643\n",
      "Epoch 968/1000\n",
      "84/84 [==============================] - 0s 770us/sample - loss: 0.0639 - acc: 0.9524\n",
      "Epoch 969/1000\n",
      "84/84 [==============================] - 0s 772us/sample - loss: 0.0514 - acc: 0.9881\n",
      "Epoch 970/1000\n",
      "84/84 [==============================] - 0s 754us/sample - loss: 0.0570 - acc: 0.9762\n",
      "Epoch 971/1000\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0555 - acc: 0.9762\n",
      "Epoch 972/1000\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0539 - acc: 0.9643\n",
      "Epoch 973/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0657 - acc: 0.9643\n",
      "Epoch 974/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0391 - acc: 0.9762\n",
      "Epoch 975/1000\n",
      "84/84 [==============================] - 0s 937us/sample - loss: 0.2727 - acc: 0.9167\n",
      "Epoch 976/1000\n",
      "84/84 [==============================] - 0s 960us/sample - loss: 0.0621 - acc: 0.9762\n",
      "Epoch 977/1000\n",
      "84/84 [==============================] - 0s 909us/sample - loss: 0.0493 - acc: 0.9524\n",
      "Epoch 978/1000\n",
      "84/84 [==============================] - 0s 844us/sample - loss: 0.0405 - acc: 0.9881\n",
      "Epoch 979/1000\n",
      "84/84 [==============================] - 0s 979us/sample - loss: 0.2310 - acc: 0.9167\n",
      "Epoch 980/1000\n",
      "84/84 [==============================] - 0s 912us/sample - loss: 0.0443 - acc: 0.9762\n",
      "Epoch 981/1000\n",
      "84/84 [==============================] - 0s 897us/sample - loss: 0.0365 - acc: 0.9762\n",
      "Epoch 982/1000\n",
      "84/84 [==============================] - 0s 933us/sample - loss: 0.0452 - acc: 0.9762\n",
      "Epoch 983/1000\n",
      "84/84 [==============================] - 0s 824us/sample - loss: 0.0615 - acc: 0.9643\n",
      "Epoch 984/1000\n",
      "84/84 [==============================] - 0s 848us/sample - loss: 0.0539 - acc: 0.9881\n",
      "Epoch 985/1000\n",
      "84/84 [==============================] - 0s 824us/sample - loss: 0.0358 - acc: 0.9762\n",
      "Epoch 986/1000\n",
      "84/84 [==============================] - 0s 785us/sample - loss: 0.0440 - acc: 0.9643\n",
      "Epoch 987/1000\n",
      "84/84 [==============================] - 0s 752us/sample - loss: 0.0525 - acc: 0.9643\n",
      "Epoch 988/1000\n",
      "84/84 [==============================] - 0s 757us/sample - loss: 0.1166 - acc: 0.9405\n",
      "Epoch 989/1000\n",
      "84/84 [==============================] - 0s 812us/sample - loss: 0.0698 - acc: 0.9643\n",
      "Epoch 990/1000\n",
      "84/84 [==============================] - 0s 849us/sample - loss: 0.0374 - acc: 0.9762\n",
      "Epoch 991/1000\n",
      "84/84 [==============================] - 0s 838us/sample - loss: 0.0464 - acc: 0.9762\n",
      "Epoch 992/1000\n",
      "84/84 [==============================] - 0s 818us/sample - loss: 0.1188 - acc: 0.9524\n",
      "Epoch 993/1000\n",
      "84/84 [==============================] - 0s 855us/sample - loss: 0.0499 - acc: 1.0000\n",
      "Epoch 994/1000\n",
      "84/84 [==============================] - 0s 807us/sample - loss: 0.0537 - acc: 0.9643\n",
      "Epoch 995/1000\n",
      "84/84 [==============================] - 0s 786us/sample - loss: 0.0493 - acc: 0.9643\n",
      "Epoch 996/1000\n",
      "84/84 [==============================] - 0s 886us/sample - loss: 0.0399 - acc: 0.9762\n",
      "Epoch 997/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.0497 - acc: 0.9643\n",
      "Epoch 998/1000\n",
      "84/84 [==============================] - 0s 918us/sample - loss: 0.0439 - acc: 0.9643\n",
      "Epoch 999/1000\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.1210 - acc: 0.9762\n",
      "Epoch 1000/1000\n",
      "84/84 [==============================] - 0s 803us/sample - loss: 0.0796 - acc: 0.9643\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=1000,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        28\n",
      "   macro avg       1.00      1.00      1.00        28\n",
      "weighted avg       1.00      1.00      1.00        28\n",
      " samples avg       1.00      1.00      1.00        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "y_pred_val = np.where(y_pred_proba_val >0.5, 1, 0)\n",
    "print(classification_report(y_val,y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.89      0.89      0.89         9\n",
      "\n",
      "   micro avg       0.95      0.95      0.95        38\n",
      "   macro avg       0.94      0.94      0.94        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      " samples avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                630       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 993\n",
      "Trainable params: 993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.6788 - acc: 0.7381\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 803us/sample - loss: 0.2743 - acc: 0.8571\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 855us/sample - loss: 0.2968 - acc: 0.8810\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 803us/sample - loss: 0.2624 - acc: 0.8690\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 799us/sample - loss: 0.1710 - acc: 0.9286\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 780us/sample - loss: 0.2240 - acc: 0.8810\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 786us/sample - loss: 0.5024 - acc: 0.7500\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 836us/sample - loss: 0.2327 - acc: 0.9286\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 867us/sample - loss: 0.2379 - acc: 0.9048\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 811us/sample - loss: 0.1979 - acc: 0.8929\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 774us/sample - loss: 0.1456 - acc: 0.9405\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 814us/sample - loss: 0.2640 - acc: 0.9167\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 866us/sample - loss: 0.2058 - acc: 0.9167\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 785us/sample - loss: 0.2366 - acc: 0.8810\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 773us/sample - loss: 0.1186 - acc: 0.9643\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 778us/sample - loss: 0.1347 - acc: 0.9405\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 842us/sample - loss: 0.1544 - acc: 0.9405\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 869us/sample - loss: 0.1964 - acc: 0.9167\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 924us/sample - loss: 0.1803 - acc: 0.9524\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 809us/sample - loss: 0.0890 - acc: 0.9524\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 871us/sample - loss: 0.0904 - acc: 0.9762\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 947us/sample - loss: 0.1504 - acc: 0.9643\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 930us/sample - loss: 0.1226 - acc: 0.9524\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 944us/sample - loss: 0.2070 - acc: 0.8929\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 939us/sample - loss: 0.1331 - acc: 0.9524\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 991us/sample - loss: 0.1379 - acc: 0.9524\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 908us/sample - loss: 0.0605 - acc: 0.9762\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 936us/sample - loss: 0.1586 - acc: 0.9524\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 788us/sample - loss: 0.1219 - acc: 0.9643\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 951us/sample - loss: 0.1395 - acc: 0.9524\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 889us/sample - loss: 0.1273 - acc: 0.9405\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 776us/sample - loss: 0.0856 - acc: 0.9643\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 831us/sample - loss: 0.1138 - acc: 0.9762\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 863us/sample - loss: 0.1975 - acc: 0.8929\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 893us/sample - loss: 0.1315 - acc: 0.9643\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 777us/sample - loss: 0.0927 - acc: 0.9524\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 706us/sample - loss: 0.1650 - acc: 0.9405\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 730us/sample - loss: 0.1243 - acc: 0.9405\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 765us/sample - loss: 0.0741 - acc: 0.9524\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 763us/sample - loss: 0.0817 - acc: 0.9643\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 735us/sample - loss: 0.0784 - acc: 0.9762\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 701us/sample - loss: 0.0840 - acc: 0.9643\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.1310 - acc: 0.9524\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0915 - acc: 0.9762\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.1217 - acc: 0.9286\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 720us/sample - loss: 0.0967 - acc: 0.9524\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.1479 - acc: 0.9405\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 712us/sample - loss: 0.0943 - acc: 0.9643\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0534 - acc: 0.9762\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 740us/sample - loss: 0.2093 - acc: 0.9524\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 756us/sample - loss: 0.0868 - acc: 0.9643\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0738 - acc: 0.9762\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.1055 - acc: 0.9643\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0753 - acc: 0.9881\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 715us/sample - loss: 0.0681 - acc: 0.9762\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.2360 - acc: 0.9048\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0796 - acc: 0.9762\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 711us/sample - loss: 0.0401 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 704us/sample - loss: 0.2969 - acc: 0.9286\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.1915 - acc: 0.9286\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 731us/sample - loss: 0.0861 - acc: 0.9762\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.1605 - acc: 0.9405\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0920 - acc: 0.9524\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.1185 - acc: 0.9286\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0807 - acc: 0.9524\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.1697 - acc: 0.9167\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 748us/sample - loss: 0.0850 - acc: 0.9524\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 746us/sample - loss: 0.0478 - acc: 0.9762\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 727us/sample - loss: 0.1477 - acc: 0.9524\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.4558 - acc: 0.8810\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.1372 - acc: 0.9524\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0786 - acc: 0.9762\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 723us/sample - loss: 0.0778 - acc: 0.9881\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 736us/sample - loss: 0.1548 - acc: 0.9524\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 701us/sample - loss: 0.0672 - acc: 0.9643\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 743us/sample - loss: 0.0852 - acc: 0.9524\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.1171 - acc: 0.9762\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 729us/sample - loss: 0.0691 - acc: 0.9524\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.0608 - acc: 0.9762\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 739us/sample - loss: 0.1791 - acc: 0.9524\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 717us/sample - loss: 0.3214 - acc: 0.8929\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.0540 - acc: 0.9881\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 699us/sample - loss: 0.0875 - acc: 0.9643\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.2429 - acc: 0.9405\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 719us/sample - loss: 0.1781 - acc: 0.9167\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 713us/sample - loss: 0.0918 - acc: 0.9524\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 721us/sample - loss: 0.0829 - acc: 0.9643\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 747us/sample - loss: 0.1942 - acc: 0.9286\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.1617 - acc: 0.9643\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 705us/sample - loss: 0.0946 - acc: 0.9643\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.0558 - acc: 0.9881\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 725us/sample - loss: 0.1219 - acc: 0.9405\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 718us/sample - loss: 0.1229 - acc: 0.9405\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 744us/sample - loss: 0.0715 - acc: 0.9524\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 734us/sample - loss: 0.1044 - acc: 0.9643\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 724us/sample - loss: 0.0846 - acc: 0.9524\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 722us/sample - loss: 0.1701 - acc: 0.9405\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 745us/sample - loss: 0.0693 - acc: 0.9643\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 737us/sample - loss: 0.0709 - acc: 0.9524\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 738us/sample - loss: 0.1268 - acc: 0.9762\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(20, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(30, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.softmax))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=100,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        28\n",
      "   macro avg       1.00      1.00      1.00        28\n",
      "weighted avg       1.00      1.00      1.00        28\n",
      " samples avg       1.00      1.00      1.00        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "y_pred_val = np.where(y_pred_proba_val >0.5, 1, 0)\n",
    "print(classification_report(y_val,y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      " samples avg       0.97      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 【問題5】House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"/Users/naoki/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing[[\"GrLivArea\", \"YearBuilt\"]]\n",
    "X = np.log(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing[\"SalePrice\"]\n",
    "y = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test,\n",
    " y_train, y_test) = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0,\n",
    ")\n",
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "821/821 [==============================] - 1s 880us/sample - loss: 121.5179\n",
      "Epoch 2/50\n",
      "821/821 [==============================] - 1s 752us/sample - loss: 121.5043\n",
      "Epoch 3/50\n",
      "821/821 [==============================] - 1s 731us/sample - loss: 121.5043\n",
      "Epoch 4/50\n",
      "821/821 [==============================] - 1s 749us/sample - loss: 121.5043\n",
      "Epoch 5/50\n",
      "821/821 [==============================] - 1s 729us/sample - loss: 121.5043\n",
      "Epoch 6/50\n",
      "821/821 [==============================] - 1s 723us/sample - loss: 121.5043\n",
      "Epoch 7/50\n",
      "821/821 [==============================] - 1s 746us/sample - loss: 121.5043\n",
      "Epoch 8/50\n",
      "821/821 [==============================] - 1s 734us/sample - loss: 121.5043\n",
      "Epoch 9/50\n",
      "821/821 [==============================] - 1s 739us/sample - loss: 121.5043\n",
      "Epoch 10/50\n",
      "821/821 [==============================] - 1s 732us/sample - loss: 121.5043\n",
      "Epoch 11/50\n",
      "821/821 [==============================] - 1s 767us/sample - loss: 121.5043\n",
      "Epoch 12/50\n",
      "821/821 [==============================] - 1s 797us/sample - loss: 121.5043\n",
      "Epoch 13/50\n",
      "821/821 [==============================] - 1s 777us/sample - loss: 121.5043\n",
      "Epoch 14/50\n",
      "821/821 [==============================] - 1s 785us/sample - loss: 121.5043\n",
      "Epoch 15/50\n",
      "821/821 [==============================] - 1s 811us/sample - loss: 121.5043\n",
      "Epoch 16/50\n",
      "821/821 [==============================] - 1s 786us/sample - loss: 121.5043\n",
      "Epoch 17/50\n",
      "821/821 [==============================] - 1s 795us/sample - loss: 121.5043\n",
      "Epoch 18/50\n",
      "821/821 [==============================] - 1s 827us/sample - loss: 121.5043s - lo\n",
      "Epoch 19/50\n",
      "821/821 [==============================] - 1s 776us/sample - loss: 121.5043\n",
      "Epoch 20/50\n",
      "821/821 [==============================] - 1s 706us/sample - loss: 121.5043\n",
      "Epoch 21/50\n",
      "821/821 [==============================] - 1s 702us/sample - loss: 121.5043\n",
      "Epoch 22/50\n",
      "821/821 [==============================] - 1s 783us/sample - loss: 121.5043\n",
      "Epoch 23/50\n",
      "821/821 [==============================] - 1s 726us/sample - loss: 121.5043\n",
      "Epoch 24/50\n",
      "821/821 [==============================] - 1s 729us/sample - loss: 121.5043\n",
      "Epoch 25/50\n",
      "821/821 [==============================] - 1s 746us/sample - loss: 121.5043\n",
      "Epoch 26/50\n",
      "821/821 [==============================] - 1s 769us/sample - loss: 121.5043\n",
      "Epoch 27/50\n",
      "821/821 [==============================] - 1s 775us/sample - loss: 121.5043\n",
      "Epoch 28/50\n",
      "821/821 [==============================] - 1s 747us/sample - loss: 121.5043\n",
      "Epoch 29/50\n",
      "821/821 [==============================] - 1s 768us/sample - loss: 121.5043\n",
      "Epoch 30/50\n",
      "821/821 [==============================] - 1s 813us/sample - loss: 121.5043\n",
      "Epoch 31/50\n",
      "821/821 [==============================] - 1s 770us/sample - loss: 121.5043\n",
      "Epoch 32/50\n",
      "821/821 [==============================] - 1s 826us/sample - loss: 121.5043\n",
      "Epoch 33/50\n",
      "821/821 [==============================] - 1s 892us/sample - loss: 121.5043\n",
      "Epoch 34/50\n",
      "821/821 [==============================] - 1s 810us/sample - loss: 121.5043\n",
      "Epoch 35/50\n",
      "821/821 [==============================] - 1s 965us/sample - loss: 121.5043\n",
      "Epoch 36/50\n",
      "821/821 [==============================] - 1s 854us/sample - loss: 121.5043\n",
      "Epoch 37/50\n",
      "821/821 [==============================] - 1s 786us/sample - loss: 121.5043\n",
      "Epoch 38/50\n",
      "821/821 [==============================] - 1s 832us/sample - loss: 121.5043\n",
      "Epoch 39/50\n",
      "821/821 [==============================] - 1s 745us/sample - loss: 121.5043\n",
      "Epoch 40/50\n",
      "821/821 [==============================] - 1s 746us/sample - loss: 121.5043\n",
      "Epoch 41/50\n",
      "821/821 [==============================] - 1s 758us/sample - loss: 121.5043\n",
      "Epoch 42/50\n",
      "821/821 [==============================] - 1s 736us/sample - loss: 121.5043\n",
      "Epoch 43/50\n",
      "821/821 [==============================] - 1s 735us/sample - loss: 121.5043\n",
      "Epoch 44/50\n",
      "821/821 [==============================] - 1s 732us/sample - loss: 121.5043\n",
      "Epoch 45/50\n",
      "821/821 [==============================] - 1s 728us/sample - loss: 121.5043s - loss: 12\n",
      "Epoch 46/50\n",
      "821/821 [==============================] - 1s 725us/sample - loss: 121.5043\n",
      "Epoch 47/50\n",
      "821/821 [==============================] - 1s 732us/sample - loss: 121.5043\n",
      "Epoch 48/50\n",
      "821/821 [==============================] - 1s 735us/sample - loss: 121.5043\n",
      "Epoch 49/50\n",
      "821/821 [==============================] - 1s 737us/sample - loss: 121.5043\n",
      "Epoch 50/50\n",
      "821/821 [==============================] - 1s 737us/sample - loss: 121.5043\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(20, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(20, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.09880071913767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred_proba_val = model.predict(X_val)\n",
    "print(mean_squared_error(y_val, y_pred_proba_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.79743138298002\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEDCAYAAAA849PJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAaJ0lEQVR4nO3df5BdZ33f8fdn791d8aO2g7xhgqR0xUgUFpoYvKMaTBkHgZFTWrmpNVlNaQ1VRg1jFWjiaeSmNoMST+MZiiFjA6NBKorrIDsCkm2iojDICTFDZa1/YFuSlaxlgRa58YKEwIaVvNK3f5znrq6u7u49+0NS9j6f14zG5z7nOWefZ7yzn3ue55zzKCIwM7P8dFzqBpiZ2aXhADAzy5QDwMwsUw4AM7NMOQDMzDLlADAzy1RbBYCkNZL2STojqX+SOkskPSTpQKr70VbHS+qV9DNJT6R/ny/RlvslHZT0tKStkjrnppdmZnNj3gaApOskfbGh+Gng14BvTnHoOPDbEfEm4BrgFkl9JY5/NiKuSv9+s0QT7wfeCPxT4BXAb5Q4xszsoqle6gbMpYg4ACBpqjrPA8+n7Z9IOgAsAvaXOb6RpOuBTwDdwLPAhyLixYjYWVfnEWDxdPtjZnYhzdsrgLkgqRd4K7CnRPWlkh6X9NeS/nk6/krgvwHviYi3AUPAbzX8jE7g3wFfm8Omm5nN2ry7ApC0h+Lb9quB10h6Iu36nYjYNY3zvBr4MvCxiPhxi+rPA78YET+UdDXwp5LeTDGE1Ad8K101dAHfbjj2s8A3I+JvyrbNzOximHcBEBH/DIo5AOCDEfHB6Z4jfSv/MnB/RHylxM88CZxM249KehZ4AyDg6xGxdpKf83GgB/iP022jmdmFlt0QkIqv6luAAxHxqZLH9EiqpO3XA8uBQ8D/Ba6VtCzte6WkN6Tt3wDeB6yNiDNz3xMzs9lpqwCQ9K8ljQBvB/5C0q5U/jpJtUnZaynG5N9dd1vnr051PPAu4ElJ3wF2AL8ZEcciYhT4IPAlSU9SBMIb0zGfB14LfDv9jDsucPfNzKZFfh20mVme2uoKwMzMyptXk8BXXnll9Pb2XupmmJnNK48++ugPIqKnsXxeBUBvby9DQ0OXuhlmZvOKpO82K/cQkJlZphwAZmaZcgCYmWXKAWBmlqlSASBpVXq3/bCkjU32d0t6IO3fk16yhqSF6d37L0q6p+GYtZKekvSkpK+lF6uZmdlF0jIA0isQ7gVuoHjx2dq69+fXrAOOR8Qy4G7grlQ+BtwO3NpwzirwGeBXIuKXgCeBDbPoh5mZTVOZK4AVwHBEHIqIU8B2YHVDndXAtrS9A1gpSRHxUkQ8TBEE9ZT+vSq9m+cy4OhMO2FmZtNXJgAWAUfqPo+ksqZ1ImIcOAEsnOyEEfEy8GHgKYo//H0UL2g7j6T1koYkDY2OjpZo7vm++K3nGPyO88XMrF6ZAGi2PFbjC4TK1DlbuXgd84cpFmN5HcUQ0G3N6kbE5ojoj4j+np7zHmQr5Y8f+R47n3x+RseambWrMgEwAiyp+7yY84drJuqk8f3LgWNTnPMqgIh4Noq30T0IvKNkm6etu1rh5PjpC3V6M7N5qUwA7AWWS1oqqQsYAAYb6gwCN6ftm4DdMfVrRr8P9EmqfaV/L3CgfLOnp7vawclxv5LfzKxey3cBRcS4pA3ALqACbI2IfZI2AUMRMUgxfn+fpGGKb/4DteMlHaaY5O2SdCNwfUTsl/QJ4JuSXga+S/Fe/QtiQWeFn73sKwAzs3qlXgYXETuBnQ1ld9RtjwFrJjm2d5Lyz1MsmnLBdVc7+NHPTl2MH2VmNm9k8SRwd2cHJ1/2EJCZWb08AqBa8RyAmVmDTAKgw3cBmZk1yCgAfAVgZlYvjwDorHgOwMysQR4BkIaApn40wcwsL9kEwJmA8TMOADOzmkwCoALgeQAzszp5BEBn0c2TfhrYzGxCHgFQTQHgKwAzswmZBICHgMzMGmUSALUrAA8BmZnV5BEAaQ5gzM8CmJlNyCMAakNAngQ2M5uQSQB4EtjMrFEmAeBJYDOzRnkEQKcngc3MGuURALUhIE8Cm5lNyCQAPARkZtYokwDwEJCZWaNSASBplaSDkoYlbWyyv1vSA2n/Hkm9qXyhpIckvSjpnoZjuiRtlvS3kp6R9G/mokPNnJ0D8BWAmVlNtVUFSRXgXuC9wAiwV9JgROyvq7YOOB4RyyQNAHcBvw6MAbcDb0n/6v0u8EJEvEFSB/CaWfdmEmefA3AAmJnVlLkCWAEMR8ShiDgFbAdWN9RZDWxL2zuAlZIUES9FxMMUQdDoPwD/HSAizkTED2bUgxIqHaKzIg8BmZnVKRMAi4AjdZ9HUlnTOhExDpwAFk52QklXpM3fk/SYpD+R9NpJ6q6XNCRpaHR0tERzm+uuVjwEZGZWp0wAqElZ49JaZerUqwKLgW9FxNuAbwOfbFYxIjZHRH9E9Pf09JRobnO1ZSHNzKxQJgBGgCV1nxcDRyerI6kKXA4cm+KcPwR+Cnw1ff4T4G0l2jJj3dUOzwGYmdUpEwB7geWSlkrqAgaAwYY6g8DNafsmYHdMsQJ72ve/getS0Upg/2T150J3p4eAzMzqtbwLKCLGJW0AdgEVYGtE7JO0CRiKiEFgC3CfpGGKb/4DteMlHQYuA7ok3Qhcn+4g+p10zKeBUeBDc9u1c3kIyMzsXC0DACAidgI7G8ruqNseA9ZMcmzvJOXfBd5VtqGzVQSArwDMzGqyeBIY0l1AngMwM5uQTwB0egjIzKxePgHgISAzs3NkFAC+C8jMrF5GAeAhIDOzevkEQKcfBDMzq5dPAFQrjL3sKwAzs5qMAsCTwGZm9bILgCneUGFmlpV8AqCzWBTm1GlfBZiZQU4BUPWykGZm9fILAN8JZGYGZBUAaV1gPwtgZgbkFACdHgIyM6uXTwB4CMjM7Bz5BECnh4DMzOrlEwC+C8jM7BwZBUDtCsABYGYGWQVAbQ7AQ0BmZpBRACzwXUBmZucoFQCSVkk6KGlY0sYm+7slPZD275HUm8oXSnpI0ouS7pnk3IOSnp5NJ8rwEJCZ2blaBoCkCnAvcAPQB6yV1NdQbR1wPCKWAXcDd6XyMeB24NZJzv1rwIsza/r0nJ0E9hCQmRmUuwJYAQxHxKGIOAVsB1Y31FkNbEvbO4CVkhQRL0XEwxRBcA5JrwZ+C/j9Gbd+GiauAPwcgJkZUC4AFgFH6j6PpLKmdSJiHDgBLGxx3t8D/gfw06kqSVovaUjS0OjoaInmNucngc3MzlUmANSkrPGl+mXqnK0sXQUsi4ivtvrhEbE5Ivojor+np6dV9Ul1VTwEZGZWr0wAjABL6j4vBo5OVkdSFbgcODbFOd8OXC3pMPAw8AZJf1WuyTPT0SG6Kl4VzMyspkwA7AWWS1oqqQsYAAYb6gwCN6ftm4DdMcXSWxHxuYh4XUT0Au8E/jYirptu46eru+qF4c3MaqqtKkTEuKQNwC6gAmyNiH2SNgFDETEIbAHukzRM8c1/oHZ8+pZ/GdAl6Ubg+ojYP/ddaa27s8NDQGZmScsAAIiIncDOhrI76rbHgDWTHNvb4tyHgbeUacdsdVcrHgIyM0uyeRIYzi4Mb2ZmmQVAV7WDMb8LyMwMyCwAujs9BGRmVpNXAFQ7/DZQM7MkvwDwFYCZGZBdAHgIyMysJq8A8HMAZmYT8goAPwlsZjYhswDwEJCZWU1WAbDAQ0BmZhOyCgBfAZiZnZVZAHRwavwMU7yo1MwsG3kFgFcFMzObkFcA1NYFdgCYmeUWAF4W0sysJs8A8LMAZmaZBUCnh4DMzGryCgAPAZmZTcg0AHwFYGaWWQCkISDPAZiZlQsASaskHZQ0LGljk/3dkh5I+/dI6k3lCyU9JOlFSffU1X+lpL+Q9IykfZL+YK46NJWzzwF4CMjMrGUASKoA9wI3AH3AWkl9DdXWAccjYhlwN3BXKh8DbgdubXLqT0bEG4G3AtdKumFmXSjPQ0BmZmeVuQJYAQxHxKGIOAVsB1Y31FkNbEvbO4CVkhQRL0XEwxRBMCEifhoRD6XtU8BjwOJZ9KMUPwhmZnZWmQBYBByp+zySyprWiYhx4ASwsEwDJF0B/EvgG5PsXy9pSNLQ6OhomVNO6uxzAB4CMjMrEwBqUtb4NrUydc4/sVQFvgT8YUQcalYnIjZHRH9E9Pf09LRs7FRqcwBjvgIwMysVACPAkrrPi4Gjk9VJf9QvB46VOPdm4O8i4tMl6s7a2buAfAVgZlYmAPYCyyUtldQFDACDDXUGgZvT9k3A7mjxzmVJv08RFB+bXpNnzpPAZmZnVVtViIhxSRuAXUAF2BoR+yRtAoYiYhDYAtwnaZjim/9A7XhJh4HLgC5JNwLXAz8Gfhd4BnhMEsA9EfGFuexcIweAmdlZLQMAICJ2Ajsbyu6o2x4D1kxybO8kp202b3BBSaKr6mUhzcwgsyeBobgK8JPAZmZZBoDXBTYzgywDwENAZmaQYQAs6OzwFYCZGRkGQHe14jkAMzNyDIBODwGZmUGOAVD1EJCZGWQZAL4LyMwMsgyADr8LyMyMHAOgs8IpXwGYmWUYAJ4DMDMDsg0ADwGZmWUYAH4OwMwMcgwAPwlsZgbkGADVDk6dPsOZMy1XrDQza2sZBkCxLOSp074KMLO8ZRgAaVUwzwOYWebyC4DO2rKQvhPIzPKWXwCkISBPBJtZ7jIMgKLLY34dhJllrlQASFol6aCkYUkbm+zvlvRA2r9HUm8qXyjpIUkvSrqn4ZirJT2VjvlDSRdlkfiJOQBfAZhZ5loGgKQKcC9wA9AHrJXU11BtHXA8IpYBdwN3pfIx4Hbg1ian/hywHlie/q2aSQemq7uzNgTkKwAzy1uZK4AVwHBEHIqIU8B2YHVDndXAtrS9A1gpSRHxUkQ8TBEEEyT9AnBZRHw7IgL4I+DG2XSkLN8FZGZWKBMAi4AjdZ9HUlnTOhExDpwAFrY450iLcwIgab2kIUlDo6OjJZo7NQ8BmZkVygRAs7H5xsdoy9SZUf2I2BwR/RHR39PTM8Upyzl7F5CHgMwsb2UCYARYUvd5MXB0sjqSqsDlwLEW51zc4pwXxNnnAHwFYGZ5KxMAe4HlkpZK6gIGgMGGOoPAzWn7JmB3GttvKiKeB34i6Zp098+/B/5s2q2fgQW1SWDPAZhZ5qqtKkTEuKQNwC6gAmyNiH2SNgFDETEIbAHukzRM8c1/oHa8pMPAZUCXpBuB6yNiP/Bh4IvAK4D/k/5dcGfnADwEZGZ5axkAABGxE9jZUHZH3fYYsGaSY3snKR8C3lK2oXPFk8BmZoUMnwT2qyDMzCDDAOisCAlO+lUQZpa57AJAkheGNzMjwwCAtC6wA8DMMpdpAHT4LiAzy16eAdDZ4ecAzCx7eQaAh4DMzHINAA8BmZllHAC+AjCzvGUaABXPAZhZ9vIMgE4PAZmZ5RkAHgIyM8s1AHwXkJlZpgHQwZjfBWRmmcszADo9BGRmlmcAVCt+G6iZZS/TAPAVgJlZpgFQYfxMMH7aIWBm+cozADqLbp9yAJhZxvIMgNq6wH4a2MwyVioAJK2SdFDSsKSNTfZ3S3og7d8jqbdu322p/KCk99WV/2dJ+yQ9LelLkhbMRYfKWNDpdYHNzFoGgKQKcC9wA9AHrJXU11BtHXA8IpYBdwN3pWP7gAHgzcAq4LOSKpIWAR8B+iPiLUAl1bsoJq4A/DoIM8tYmSuAFcBwRByKiFPAdmB1Q53VwLa0vQNYKUmpfHtEnIyI54DhdD6AKvAKSVXglcDR2XWlvO6qrwDMzMoEwCLgSN3nkVTWtE5EjAMngIWTHRsR3wc+CXwPeB44ERF/2eyHS1ovaUjS0OjoaInmtuY5ADOzcgGgJmVRsk7Tckk/R3F1sBR4HfAqSR9o9sMjYnNE9EdEf09PT4nmtla7C8hDQGaWszIBMAIsqfu8mPOHaybqpCGdy4FjUxz7HuC5iBiNiJeBrwDvmEkHZsJDQGZm5QJgL7Bc0lJJXRSTtYMNdQaBm9P2TcDuiIhUPpDuEloKLAceoRj6uUbSK9NcwUrgwOy7U44ngc3MionYKUXEuKQNwC6Ku3W2RsQ+SZuAoYgYBLYA90kapvjmP5CO3SfpQWA/MA7cEhGngT2SdgCPpfLHgc1z373mJoaAPAdgZhlrGQAAEbET2NlQdkfd9hiwZpJj7wTubFL+ceDj02nsXPEQkJlZ7k8CewjIzDKWeQD4CsDM8pVnANReBeE5ADPLWJ4B4CEgM7M8A6DaITrkISAzy1uWASCpWBbSAWBmGcsyACAtDO91gc0sY/kGgNcFNrPMZRwAFcZ8BWBmGcs4AHwFYGZ5yzcAOh0AZpa3fAOgWvFzAGaWtYwDoMNPAptZ1vIOAA8BmVnGMg4ADwGZWd6yDYAFngQ2s8xlGwDd1YrnAMwsa/kGQGeHh4DMLGv5BoAngc0scxkHgN8GamZ5KxUAklZJOihpWNLGJvu7JT2Q9u+R1Fu377ZUflDS++rKr5C0Q9Izkg5IevtcdKis7moHp88E46cdAmaWp5YBIKkC3AvcAPQBayX1NVRbBxyPiGXA3cBd6dg+YAB4M7AK+Gw6H8BngK9FxBuBXwYOzL475XV3el1gM8tbmSuAFcBwRByKiFPAdmB1Q53VwLa0vQNYKUmpfHtEnIyI54BhYIWky4B3AVsAIuJURPxo9t0pr7ua1gV2AJhZpsoEwCLgSN3nkVTWtE5EjAMngIVTHPt6YBT4n5Iel/QFSa9q9sMlrZc0JGlodHS0RHPL8brAZpa7MgGgJmVRss5k5VXgbcDnIuKtwEvAeXMLABGxOSL6I6K/p6enRHPLmRgC8rMAZpapMgEwAiyp+7wYODpZHUlV4HLg2BTHjgAjEbEnle+gCISLxkNAZpa7MgGwF1guaamkLopJ3cGGOoPAzWn7JmB3REQqH0h3CS0FlgOPRMT/A45I+ifpmJXA/ln2ZVo8BGRmuau2qhAR45I2ALuACrA1IvZJ2gQMRcQgxWTufZKGKb75D6Rj90l6kOKP+zhwS0TU/uL+J+D+FCqHgA/Ncd+m5CsAM8tdywAAiIidwM6GsjvqtseANZMceydwZ5PyJ4D+6TR2LnkOwMxyl/GTwB4CMrO8ZRwAHgIys7xlHABF18de9hWAmeUp3wDwqyDMLHP5BkBtCMhXAGaWqYwDwFcAZpY3B4ADwMwylW0AVCsdVDrk20DNLFvZBgCkZSH9IJiZZSrrAFjQ6WUhzSxfWQdAsTC8h4DMLE8OAF8BmFmmMg+AiucAzCxbeQdAp4eAzCxfeQeAh4DMLGOZB4DvAjKzfGUeAB4CMrN85R0AnX4QzMzyVWpJyHbVXa1w+Icv8d5P/fWlboqZ2ZT+/CPvnHiL8VzJOgDWXL3YQ0BmNi8Izfk5SwWApFXAZ4AK8IWI+IOG/d3AHwFXAz8Efj0iDqd9twHrgNPARyJiV91xFWAI+H5EvH/WvZmmdyy7kncsu/Ji/1gzs38QWs4BpD/S9wI3AH3AWkl9DdXWAccjYhlwN3BXOrYPGADeDKwCPpvOV/NR4MBsO2FmZtNXZhJ4BTAcEYci4hSwHVjdUGc1sC1t7wBWSlIq3x4RJyPiOWA4nQ9Ji4F/AXxh9t0wM7PpKhMAi4AjdZ9HUlnTOhExDpwAFrY49tPAfwGmvA1H0npJQ5KGRkdHSzTXzMzKKBMAzWYeomSdpuWS3g+8EBGPtvrhEbE5Ivojor+np6d1a83MrJQyATACLKn7vBg4OlkdSVXgcuDYFMdeC/wrSYcphpTeLel/zaD9ZmY2Q2UCYC+wXNJSSV0Uk7qDDXUGgZvT9k3A7oiIVD4gqVvSUmA58EhE3BYRiyOiN51vd0R8YA76Y2ZmJbW8DTQixiVtAHZR3Aa6NSL2SdoEDEXEILAFuE/SMMU3/4F07D5JDwL7gXHglojwjfdmZv8AqPiiPj/09/fH0NDQpW6Gmdm8IunRiOg/r3w+BYCkUeC7Mzz8SuAHc9ic+cL9zov7nZey/f7HEXHeXTTzKgBmQ9JQswRsd+53XtzvvMy231m/DdTMLGcOADOzTOUUAJsvdQMuEfc7L+53XmbV72zmAMzM7Fw5XQGYmVkdB4CZWabaPgAkrZJ0UNKwpI2Xuj0XkqStkl6Q9HRd2WskfV3S36X//tylbOOFIGmJpIckHZC0T9JHU3lb913SAkmPSPpO6vcnUvlSSXtSvx9Ir3BpO5Iqkh6X9Ofpc9v3W9JhSU9JekLSUCqb8e95WwdAycVs2skXKRbeqbcR+EZELAe+kT63m3HgtyPiTcA1wC3p/3O79/0k8O6I+GXgKmCVpGsoFmS6O/X7OMWCTe2ocUGpXPr9KxFxVd39/zP+PW/rAKDcYjZtIyK+SfEupnr1i/VsA268qI26CCLi+Yh4LG3/hOKPwiLavO9ReDF97Ez/Ang3xcJM0Ib9hvMXlEoLULV9vycx49/zdg+AMovZtLvXRsTzUPyhBH7+ErfngpLUC7wV2EMGfU/DIE8ALwBfB54FfpQWZoL2/Z1vXFBqIXn0O4C/lPSopPWpbMa/56UWhZ/HyixmY21C0quBLwMfi4gfF18K21t6u+5Vkq4Avgq8qVm1i9uqC6t+QSlJ19WKm1Rtq34n10bEUUk/D3xd0jOzOVm7XwGUWcym3f29pF8ASP994RK354KQ1Enxx//+iPhKKs6i7wAR8SPgryjmQK5ICzNBe/7On7egFMUVQbv3m4g4mv77AkXgr2AWv+ftHgBlFrNpd/WL9dwM/NklbMsFkcZ/twAHIuJTdbvauu+SetI3fyS9AngPxfzHQxQLM0Eb9nuSBaX+LW3eb0mvkvSPatvA9cDTzOL3vO2fBJb0qxTfDmqL2dx5iZt0wUj6EnAdxSti/x74OPCnwIPALwLfA9ZERONE8bwm6Z3A3wBPcXZM+L9SzAO0bd8l/RLFpF+F4svcgxGxSdLrKb4ZvwZ4HPhARJy8dC29cNIQ0K0R8f5273fq31fTxyrwxxFxp6SFzPD3vO0DwMzMmmv3ISAzM5uEA8DMLFMOADOzTDkAzMwy5QAwM8uUA8DMLFMOADOzTP1/8rd9PztqjdMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           30          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           110         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           110         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           220         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           550         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 80)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            81          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,101\n",
      "Trainable params: 1,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "821/821 [==============================] - 1s 1ms/sample - loss: 1.2669\n",
      "Epoch 2/50\n",
      "821/821 [==============================] - 1s 822us/sample - loss: 0.2272\n",
      "Epoch 3/50\n",
      "821/821 [==============================] - 1s 784us/sample - loss: 0.2037\n",
      "Epoch 4/50\n",
      "821/821 [==============================] - 1s 785us/sample - loss: 0.1958\n",
      "Epoch 5/50\n",
      "821/821 [==============================] - 1s 795us/sample - loss: 0.2246\n",
      "Epoch 6/50\n",
      "821/821 [==============================] - 1s 785us/sample - loss: 0.2037s - l\n",
      "Epoch 7/50\n",
      "821/821 [==============================] - 1s 790us/sample - loss: 0.1727\n",
      "Epoch 8/50\n",
      "821/821 [==============================] - 1s 854us/sample - loss: 0.1735\n",
      "Epoch 9/50\n",
      "821/821 [==============================] - 1s 787us/sample - loss: 0.1778\n",
      "Epoch 10/50\n",
      "821/821 [==============================] - 1s 808us/sample - loss: 0.1744\n",
      "Epoch 11/50\n",
      "821/821 [==============================] - 1s 787us/sample - loss: 0.1623\n",
      "Epoch 12/50\n",
      "821/821 [==============================] - 1s 793us/sample - loss: 0.1514\n",
      "Epoch 13/50\n",
      "821/821 [==============================] - 1s 809us/sample - loss: 0.1501\n",
      "Epoch 14/50\n",
      "821/821 [==============================] - 1s 796us/sample - loss: 0.1541\n",
      "Epoch 15/50\n",
      "821/821 [==============================] - 1s 821us/sample - loss: 0.1609\n",
      "Epoch 16/50\n",
      "821/821 [==============================] - 1s 837us/sample - loss: 0.1604\n",
      "Epoch 17/50\n",
      "821/821 [==============================] - 1s 871us/sample - loss: 0.1341\n",
      "Epoch 18/50\n",
      "821/821 [==============================] - 1s 799us/sample - loss: 0.1401\n",
      "Epoch 19/50\n",
      "821/821 [==============================] - 1s 805us/sample - loss: 0.1335\n",
      "Epoch 20/50\n",
      "821/821 [==============================] - 1s 800us/sample - loss: 0.1560\n",
      "Epoch 21/50\n",
      "821/821 [==============================] - 1s 833us/sample - loss: 0.1397\n",
      "Epoch 22/50\n",
      "821/821 [==============================] - 1s 841us/sample - loss: 0.1214\n",
      "Epoch 23/50\n",
      "821/821 [==============================] - 1s 884us/sample - loss: 0.1445\n",
      "Epoch 24/50\n",
      "821/821 [==============================] - 1s 889us/sample - loss: 0.1277\n",
      "Epoch 25/50\n",
      "821/821 [==============================] - 1s 887us/sample - loss: 0.1411\n",
      "Epoch 26/50\n",
      "821/821 [==============================] - 1s 874us/sample - loss: 0.1356\n",
      "Epoch 27/50\n",
      "821/821 [==============================] - 1s 833us/sample - loss: 0.1455\n",
      "Epoch 28/50\n",
      "821/821 [==============================] - 1s 818us/sample - loss: 0.1467\n",
      "Epoch 29/50\n",
      "821/821 [==============================] - 1s 953us/sample - loss: 0.1501\n",
      "Epoch 30/50\n",
      "821/821 [==============================] - 1s 851us/sample - loss: 0.1383\n",
      "Epoch 31/50\n",
      "821/821 [==============================] - 1s 840us/sample - loss: 0.1319\n",
      "Epoch 32/50\n",
      "821/821 [==============================] - 1s 838us/sample - loss: 0.1304\n",
      "Epoch 33/50\n",
      "821/821 [==============================] - 1s 858us/sample - loss: 0.1376\n",
      "Epoch 34/50\n",
      "821/821 [==============================] - 1s 959us/sample - loss: 0.1436\n",
      "Epoch 35/50\n",
      "821/821 [==============================] - 1s 951us/sample - loss: 0.1289s - loss: 0\n",
      "Epoch 36/50\n",
      "821/821 [==============================] - 1s 815us/sample - loss: 0.1347\n",
      "Epoch 37/50\n",
      "821/821 [==============================] - 1s 822us/sample - loss: 0.1280\n",
      "Epoch 38/50\n",
      "821/821 [==============================] - 1s 873us/sample - loss: 0.1333\n",
      "Epoch 39/50\n",
      "821/821 [==============================] - 1s 888us/sample - loss: 0.1347\n",
      "Epoch 40/50\n",
      "821/821 [==============================] - 1s 827us/sample - loss: 0.1229\n",
      "Epoch 41/50\n",
      "821/821 [==============================] - 1s 806us/sample - loss: 0.1346\n",
      "Epoch 42/50\n",
      "821/821 [==============================] - 1s 862us/sample - loss: 0.1542\n",
      "Epoch 43/50\n",
      "821/821 [==============================] - 1s 856us/sample - loss: 0.1288\n",
      "Epoch 44/50\n",
      "821/821 [==============================] - 1s 832us/sample - loss: 0.1250\n",
      "Epoch 45/50\n",
      "821/821 [==============================] - 1s 872us/sample - loss: 0.1236\n",
      "Epoch 46/50\n",
      "821/821 [==============================] - 1s 903us/sample - loss: 0.1239\n",
      "Epoch 47/50\n",
      "821/821 [==============================] - 1s 832us/sample - loss: 0.1276\n",
      "Epoch 48/50\n",
      "821/821 [==============================] - 1s 874us/sample - loss: 0.1362\n",
      "Epoch 49/50\n",
      "821/821 [==============================] - 1s 802us/sample - loss: 0.1175\n",
      "Epoch 50/50\n",
      "821/821 [==============================] - 1s 817us/sample - loss: 0.1333\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "y1 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "y2 = tf.keras.layers.Dense(20, activation=tf.nn.relu)(x)\n",
    "y3 = tf.keras.layers.Dense(50, activation=tf.nn.relu)(x)\n",
    "z = tf.keras.layers.concatenate([y1, y2, y3])\n",
    "output = tf.keras.layers.Dense(1)(z)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1239332529123039\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "print(mean_squared_error(y_val, y_pred_proba_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11797413363809524\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train_one_hot, test_size=0.25, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               150500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               150500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 691,810\n",
      "Trainable params: 691,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 3s 61us/sample - loss: 0.4221 - acc: 0.8884\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 3s 60us/sample - loss: 0.2677 - acc: 0.9346\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 3s 67us/sample - loss: 0.2124 - acc: 0.9483\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 3s 67us/sample - loss: 0.2192 - acc: 0.9486\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 3s 66us/sample - loss: 0.1771 - acc: 0.9578\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 3s 63us/sample - loss: 0.1667 - acc: 0.9588\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 3s 61us/sample - loss: 0.1575 - acc: 0.9611\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1778 - acc: 0.9590\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 3s 63us/sample - loss: 0.1868 - acc: 0.9570\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 3s 63us/sample - loss: 0.1282 - acc: 0.9677\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 3s 61us/sample - loss: 0.1310 - acc: 0.9675\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1234 - acc: 0.9702\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1489 - acc: 0.9664\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1280 - acc: 0.9704\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1304 - acc: 0.9689\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1163 - acc: 0.9726\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.1190 - acc: 0.9727\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 3s 67us/sample - loss: 0.1226 - acc: 0.9709\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 3s 70us/sample - loss: 0.1018 - acc: 0.9763\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 3s 58us/sample - loss: 0.1781 - acc: 0.9647\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.0969 - acc: 0.9764\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1521 - acc: 0.9687\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 3s 58us/sample - loss: 0.1416 - acc: 0.97010s - loss: 0.1435 - ac\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 3s 61us/sample - loss: 0.1429 - acc: 0.9717\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.1409 - acc: 0.9719\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.1392 - acc: 0.9718\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 2s 55us/sample - loss: 0.1490 - acc: 0.9691\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1371 - acc: 0.9719\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.1132 - acc: 0.9756\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.1288 - acc: 0.9737\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.1743 - acc: 0.9712\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.1140 - acc: 0.9783\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.0905 - acc: 0.9802\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.1245 - acc: 0.9752\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.1628 - acc: 0.9674\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1324 - acc: 0.9741\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1106 - acc: 0.9769\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 3s 56us/sample - loss: 0.1192 - acc: 0.9768\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.1426 - acc: 0.9741\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.1293 - acc: 0.9758\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1368 - acc: 0.9727\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 2s 53us/sample - loss: 0.1004 - acc: 0.9784\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1494 - acc: 0.9733\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 2s 52us/sample - loss: 0.1242 - acc: 0.9775\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 2s 54us/sample - loss: 0.1559 - acc: 0.9703\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 3s 63us/sample - loss: 0.0821 - acc: 0.9822\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 3s 64us/sample - loss: 0.1577 - acc: 0.9728\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 3s 61us/sample - loss: 0.1619 - acc: 0.9721\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 3s 61us/sample - loss: 0.0923 - acc: 0.9816\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 3s 62us/sample - loss: 0.0786 - acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(300, activation = tf.nn.relu, input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(500, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(300, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(500, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 6, ..., 9, 7, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "np.argmax(y_pred_proba_val,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      1510\n",
      "           1       0.97      0.99      0.98      1728\n",
      "           2       0.96      0.96      0.96      1458\n",
      "           3       0.97      0.95      0.96      1548\n",
      "           4       0.96      0.97      0.97      1434\n",
      "           5       0.96      0.94      0.95      1342\n",
      "           6       0.97      0.98      0.97      1468\n",
      "           7       0.98      0.97      0.97      1535\n",
      "           8       0.93      0.96      0.94      1484\n",
      "           9       0.93      0.96      0.95      1493\n",
      "\n",
      "    accuracy                           0.96     15000\n",
      "   macro avg       0.96      0.96      0.96     15000\n",
      "weighted avg       0.96      0.96      0.96     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "y_pred_val = np.argmax(y_pred_proba_val,axis=1)\n",
    "y_val_ = np.argmax(y_val,axis=1)\n",
    "print(classification_report(y_val_,y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       980\n",
      "           1       0.98      0.99      0.98      1135\n",
      "           2       0.97      0.96      0.96      1032\n",
      "           3       0.97      0.96      0.97      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.97      0.94      0.96       892\n",
      "           6       0.96      0.97      0.97       958\n",
      "           7       0.97      0.98      0.97      1028\n",
      "           8       0.93      0.96      0.95       974\n",
      "           9       0.95      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba,axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 100,710\n",
      "Trainable params: 100,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/naoki/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/naoki/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.2835 - accuracy: 0.9123\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.1299 - accuracy: 0.9596\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0998 - accuracy: 0.9688\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0809 - accuracy: 0.9744\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0727 - accuracy: 0.9764\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0621 - accuracy: 0.9791\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0630 - accuracy: 0.9795\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.0563 - accuracy: 0.9807\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0522 - accuracy: 0.9830\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0513 - accuracy: 0.9830\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0473 - accuracy: 0.9844\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0471 - accuracy: 0.9847\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0395 - accuracy: 0.9868\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0375 - accuracy: 0.9873\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0383 - accuracy: 0.9868\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0381 - accuracy: 0.9872\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0357 - accuracy: 0.9884\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0368 - accuracy: 0.9881\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0305 - accuracy: 0.9896\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0410 - accuracy: 0.9870\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0382 - accuracy: 0.9878\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0332 - accuracy: 0.9886\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0271 - accuracy: 0.9915\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0310 - accuracy: 0.9900\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0335 - accuracy: 0.9892\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0311 - accuracy: 0.9905\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0300 - accuracy: 0.9903\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0306 - accuracy: 0.9903\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0254 - accuracy: 0.9920\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0311 - accuracy: 0.9898\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.0259 - accuracy: 0.9915\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0281 - accuracy: 0.9910\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0290 - accuracy: 0.9908\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0308 - accuracy: 0.9900\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.0274 - accuracy: 0.9913\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0293 - accuracy: 0.9912\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0233 - accuracy: 0.9921\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0263 - accuracy: 0.9915\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0318 - accuracy: 0.9896\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0275 - accuracy: 0.9917\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0217 - accuracy: 0.9928\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0212 - accuracy: 0.9934\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0243 - accuracy: 0.9924\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0285 - accuracy: 0.9908\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0294 - accuracy: 0.9911\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0275 - accuracy: 0.9915\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0190 - accuracy: 0.9941\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0194 - accuracy: 0.9934\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.0226 - accuracy: 0.9928\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.0249 - accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(784,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1510\n",
      "           1       0.99      0.99      0.99      1728\n",
      "           2       0.98      0.97      0.97      1458\n",
      "           3       0.96      0.96      0.96      1548\n",
      "           4       0.97      0.96      0.97      1434\n",
      "           5       0.96      0.95      0.95      1342\n",
      "           6       0.98      0.98      0.98      1468\n",
      "           7       0.98      0.97      0.97      1535\n",
      "           8       0.97      0.96      0.96      1484\n",
      "           9       0.94      0.97      0.95      1493\n",
      "\n",
      "    accuracy                           0.97     15000\n",
      "   macro avg       0.97      0.97      0.97     15000\n",
      "weighted avg       0.97      0.97      0.97     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_val = model.predict(X_val)\n",
    "y_pred_val = np.argmax(y_pred_proba_val,axis=1)\n",
    "y_val_ = np.argmax(y_val,axis=1)\n",
    "print(classification_report(y_val_,y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.98      0.98      1032\n",
      "           3       0.97      0.97      0.97      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.97      0.96      0.97       892\n",
      "           6       0.99      0.96      0.98       958\n",
      "           7       0.98      0.97      0.97      1028\n",
      "           8       0.97      0.96      0.97       974\n",
      "           9       0.96      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba,axis=1)\n",
    "# y_test = np.argmax(y_test,axis=1)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 784)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/naoki/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               2765100   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 2,786,926\n",
      "Trainable params: 2,786,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "K.clear_session( )\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "# x_valid = x_valid.reshape(x_valid.shape[0], 28, 28, 1)\n",
    "# x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# # 0-255の整数値を0〜1の小数に変換する\n",
    "# # MNISTって必ずこの処理入るけれど、意味あるのかな\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_valid = x_valid.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_valid /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# # one-hot vector形式に変換する\n",
    "# enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "# y_test = enc.transform(y_test[:, np.newaxis])\n",
    "# y_valid = enc.transform(y_valid[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=0,\n",
    ")\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "45000/45000 [==============================] - 32s 704us/step - loss: 0.3382 - accuracy: 0.8952\n",
      "Epoch 2/5\n",
      "45000/45000 [==============================] - 33s 727us/step - loss: 0.1863 - accuracy: 0.9449\n",
      "Epoch 3/5\n",
      "45000/45000 [==============================] - 34s 749us/step - loss: 0.1720 - accuracy: 0.9494\n",
      "Epoch 4/5\n",
      "45000/45000 [==============================] - 35s 773us/step - loss: 0.1670 - accuracy: 0.9497\n",
      "Epoch 5/5\n",
      "45000/45000 [==============================] - 35s 774us/step - loss: 0.1564 - accuracy: 0.9545\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
